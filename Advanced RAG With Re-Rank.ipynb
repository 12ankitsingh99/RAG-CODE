{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cf9e879-db13-478e-8dc6-572473ba4b41",
   "metadata": {},
   "source": [
    "# Advanced RAG With Re-rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd34fb54-03f3-4c6c-b144-725d2cc5fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U huggingface_hub\n",
    "#!pip install llama-index\n",
    "#!pip install langchain-community\n",
    "#!pip install langchain-core\n",
    "#!pip install llama-index-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f3020ef-f0af-40b4-badb-0910d09693c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "import torch\n",
    "\n",
    "input_dir_path = r\"D:/Courses/RAG Course/\"\n",
    "#Next code will read all pdf files in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6c146ab-3a1e-4217-90c3-4c2945ef6834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "loader = SimpleDirectoryReader(\n",
    "            input_dir = input_dir_path,\n",
    "            required_exts=[\".pdf\"],\n",
    "            recursive=True\n",
    "        )\n",
    "docs = loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36dda9aa-9c5e-45fa-b039-0208c981aa68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='6f2a0321-5dc7-48e1-9ff3-1c983afb7cce', embedding=None, metadata={'page_label': '1', 'file_name': 'Q-Lora.pdf', 'file_path': 'D:\\\\Courses\\\\RAG Course\\\\Q-Lora.pdf', 'file_type': 'application/pdf', 'file_size': 1110975, 'creation_date': '2024-07-18', 'last_modified_date': '2024-07-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='QL ORA: Efficient Finetuning of Quantized LLMs\\nTim Dettmers∗Artidoro Pagnoni∗Ari Holtzman\\nLuke Zettlemoyer\\nUniversity of Washington\\n{dettmers,artidoro,ahai,lsz}@cs.washington.edu\\nAbstract\\nWe present QLORA, an efficient finetuning approach that reduces memory us-\\nage enough to finetune a 65B parameter model on a single 48GB GPU while\\npreserving full 16-bit finetuning task performance. QLORAbackpropagates gradi-\\nents through a frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters (LoRA). Our best model family, which we name Guanaco , outperforms\\nall previous openly released models on the Vicuna benchmark, reaching 99.3%\\nof the performance level of ChatGPT while only requiring 24 hours of finetuning\\non a single GPU. QLORAintroduces a number of innovations to save memory\\nwithout sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that\\nis information theoretically optimal for normally distributed weights (b) Double\\nQuantization to reduce the average memory footprint by quantizing the quantization\\nconstants, and (c) Paged Optimizers to manage memory spikes. We use QLORA\\nto finetune more than 1,000 models, providing a detailed analysis of instruction\\nfollowing and chatbot performance across 8 instruction datasets, multiple model\\ntypes (LLaMA, T5), and model scales that would be infeasible to run with regular\\nfinetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA\\nfinetuning on a small high-quality dataset leads to state-of-the-art results, even\\nwhen using smaller models than the previous SoTA. We provide a detailed analysis\\nof chatbot performance based on both human and GPT-4 evaluations showing that\\nGPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Fur-\\nthermore, we find that current chatbot benchmarks are not trustworthy to accurately\\nevaluate the performance levels of chatbots. A lemon-picked analysis demonstrates\\nwhere Guanaco fails compared to ChatGPT. We release all of our models and code,\\nincluding CUDA kernels for 4-bit training.2\\n1 Introduction\\nFinetuning large language models (LLMs) is a highly effective way to improve their performance,\\n[40,63,43,62,60,37] and to add desirable or remove undesirable behaviors [ 43,2,4]. However,\\nfinetuning very large models is prohibitively expensive; regular 16-bit finetuning of a LLaMA 65B\\nparameter model [ 58] requires more than 780 GB of GPU memory. While recent quantization\\nmethods can reduce the memory footprint of LLMs [ 14,13,18,67], such techniques only work for\\ninference and break down during training [66].\\nWe demonstrate for the first time that it is possible to finetune a quantized 4-bit model without any\\nperformance degradation. Our method, QLORA, uses a novel high-precision technique to quantize\\na pretrained model to 4-bit, then adds a small set of learnable Low-rank Adapter weights [ 28]\\n∗Equal contribution.\\n2https://github.com/artidoro/qlora andhttps://github.com/TimDettmers/bitsandbytes\\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3e295886-2d49-45d3-94ea-bdcdbcb43585', embedding=None, metadata={'page_label': '2', 'file_name': 'Q-Lora.pdf', 'file_path': 'D:\\\\Courses\\\\RAG Course\\\\Q-Lora.pdf', 'file_type': 'application/pdf', 'file_size': 1110975, 'creation_date': '2024-07-18', 'last_modified_date': '2024-07-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 1: Elo ratings for a competition between\\nmodels, averaged for 10,000 random initial order-\\nings. The winner of a match is determined by\\nGPT-4 which declares which response is better for\\na given prompt of the the Vicuna benchmark. 95%\\nconfidence intervals are shown ( ±). After GPT-\\n4, Guanaco 33B and 65B win the most matches,\\nwhile Guanaco 13B scores better than Bard.\\nModel Size Elo\\nGPT-4 - 1348 ±1\\nGuanaco 65B 41 GB 1022 ±1\\nGuanaco 33B 21 GB 992 ±1\\nVicuna 13B 26 GB 974 ±1\\nChatGPT - 966 ±1\\nGuanaco 13B 10 GB 916 ±1\\nBard - 902 ±1\\nGuanaco 7B 6 GB 879 ±1that are tuned by backpropagating gradients through\\nthe quantized weights.\\nQLORAreduces the average memory requirements\\nof finetuning a 65B parameter model from >780GB\\nof GPU memory to <48GB without degrading the\\nruntime or predictive performance compared to a 16-\\nbit fully finetuned baseline. This marks a significant\\nshift in accessibility of LLM finetuning: now the\\nlargest publicly available models to date finetunable\\non a single GPU. Using QLORA, we train the Gua-\\nnaco family of models, with the second best model\\nreaching 97.8% of the performance level of ChatGPT\\non the Vicuna [ 10] benchmark, while being trainable\\nin less than 12 hours on a single consumer GPU;\\nusing a single professional GPU over 24 hours we\\nachieve 99.3% with our largest model, essentially\\nclosing the gap to ChatGPT on the Vicuna bench-\\nmark. When deployed, our smallest Guanaco model\\n(7B parameters) requires just 5 GB of memory and outperforms a 26 GB Alpaca model by more than\\n20 percentage points on the Vicuna benchmark (Table 4).\\nQLORAintroduces multiple innovations designed to reduce memory use without sacrificing per-\\nformance: (1) 4-bit NormalFloat , an information theoretically optimal quantization data type for\\nnormally distributed data that yields better empirical results than 4-bit Integers and 4-bit Floats.\\n(2)Double Quantization , a method that quantizes the quantization constants, saving an average\\nof about 0.37 bits per parameter (approximately 3 GB for a 65B model). (3) Paged Optimizers ,\\nusing NVIDIA unified memory to avoid the gradient checkpointing memory spikes that occur when\\nprocessing a mini-batch with a long sequence length. We combine these contributions into a better\\ntuned LoRA approach that includes adapters at every network layer and thereby avoids almost all of\\nthe accuracy tradeoffs seen in prior work.\\nQLORA’s efficiency enables us to perform an in-depth study of instruction finetuning and chatbot\\nperformance on model scales that would be impossible using regular finetuning due to memory\\noverhead. Therefore, we train more than 1,000 models across several instruction tuning datasets,\\nmodel architectures, and sizes between 80M to 65B parameters. In addition to showing that QLORA\\nrecovers 16-bit performance (§4) and training a state-of-the-art chatbot, Guanaco , (§5), we also\\nanalyze trends in the trained models. First, we find that data quality is far more important than\\ndataset size, e.g., a 9k sample dataset (OASST1) outperformed a 450k sample dataset (FLAN v2,\\nsubsampled) on chatbot performance, even when both are meant to support instruction following\\ngeneralization. Second, we show that strong Massive Multitask Language Understanding (MMLU)\\nbenchmark performance does not imply strong Vicuna chatbot benchmark performance and vice\\nversa—in other words, dataset suitability matters more than size for a given task.\\nFurthermore, we also provide a extensive analysis of chatbot performance that uses both human\\nraters and GPT-4 for evaluation. We use tournament-style benchmarking where models compete\\nagainst each other in matches to produce the best response for a given prompt. The winner of a\\nmatch is judged by either GPT-4 or human annotators. The tournament results are aggregated into\\nElo scores [ 16,17] which determine the ranking of chatbot performance. We find that GPT-4 and\\nhuman evaluations largely agree on the rank of model performance in the tournaments, but we also\\nfind there are instances of strong disagreement. As such, we highlight that model-based evaluation\\nwhile providing a cheap alternative to human-annotation also has its uncertainties.\\nWe augment our chatbot benchmark results with a qualitative analysis of Guanaco models (see Ap-\\npendix F). Our analysis highlights success and failure cases that were not captured by the quantitative\\nbenchmarks.\\nWe release all model generations with human and GPT-4 annotations to facilitate further study. We\\nopen-source our codebase and CUDA kernels and integrate our methods into the Hugging Face\\ntransformers stack [ 65], making them easily accessible to all. We release a collection of adapters\\nfor 7/13/33/65B size models, trained on 8 different instruction following datasets, for a total of 32\\ndifferent open sourced, finetuned models.\\n2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3c1f4425-3e67-438e-905e-47a3c2c0ad0a', embedding=None, metadata={'page_label': '3', 'file_name': 'Q-Lora.pdf', 'file_path': 'D:\\\\Courses\\\\RAG Course\\\\Q-Lora.pdf', 'file_type': 'application/pdf', 'file_size': 1110975, 'creation_date': '2024-07-18', 'last_modified_date': '2024-07-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Figure 1: Different finetuning methods and their memory requirements. QLORAimproves over LoRA by\\nquantizing the transformer model to 4-bit precision and using paged optimizers to handle memory spikes.\\n2 Background\\nBlock-wise k-bit Quantization Quantization is the process of discretizing an input from a rep-\\nresentation that holds more information to a representation with less information. It often means\\ntaking a data type with more bits and converting it to fewer bits, for example from 32-bit floats to\\n8-bit Integers. To ensure that the entire range of the low-bit data type is used, the input data type is\\ncommonly rescaled into the target data type range through normalization by the absolute maximum\\nof the input elements, which are usually structured as a tensor. For example, quantizing a 32-bit\\nFloating Point (FP32) tensor into a Int8 tensor with range [−127,127]:\\nXInt8=round\\x12127\\nabsmax (XFP32)XFP32\\x13\\n=round (cFP32·XFP32), (1)\\nwhere cis the quantization constant orquantization scale . Dequantization is the inverse:\\ndequant (cFP32,XInt8) =XInt8\\ncFP32=XFP32(2)\\nThe problem with this approach is that if a large magnitude value (i.e., an outlier) occurs in the input\\ntensor, then the quantization bins—certain bit combinations—are not utilized well with few or no\\nnumbers quantized in some bins. To prevent the outlier issue, a common approach is to chunk the\\ninput tensor into blocks that are independently quantized, each with their own quantization constant c.\\nThis can be formalized as follows: We chunk the input tensor X∈Rb×hintoncontiguous blocks of\\nsizeBby flattening the input tensor and slicing the linear segment into n= (b×h)/Bblocks. We\\nquantize these blocks independently with Equation 1 to create a quantized tensor and nquantization\\nconstants ci.\\nLow-rank Adapters Low-rank Adapter (LoRA) finetuning [ 28] is a method that reduces memory\\nrequirements by using a small set of trainable parameters, often termed adapters, while not updating\\nthe full model parameters which remain fixed. Gradients during stochastic gradient descent are\\npassed through the fixed pretrained model weights to the adapter, which is updated to optimize the\\nloss function. LoRA augments a linear projection through an additional factorized projection. Given\\na projection XW =YwithX∈Rb×h,W∈Rh×oLoRA computes:\\nY=XW +sXL 1L2, (3)\\nwhereL1∈Rh×randL2∈Rr×o, and sis a scalar.\\n3 QL ORA Finetuning\\nQLORAachieves high-fidelity 4-bit finetuning via two techniques we propose—4-bit NormalFloat\\n(NF4) quantization and Double Quantization. Additionally, we introduce Paged Optimizers, to\\n3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='529ba3bf-955d-49aa-98a9-946d002f5754', embedding=None, metadata={'page_label': '4', 'file_name': 'Q-Lora.pdf', 'file_path': 'D:\\\\Courses\\\\RAG Course\\\\Q-Lora.pdf', 'file_type': 'application/pdf', 'file_size': 1110975, 'creation_date': '2024-07-18', 'last_modified_date': '2024-07-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='prevent memory spikes during gradient checkpointing from causing out-of-memory errors that have\\ntraditionally made finetuning on a single machine difficult for large models.\\nQLORAhas one low-precision storage data type, in our case usually 4-bit, and one computation data\\ntype that is usually BFloat16. In practice, this means whenever a QLORAweight tensor is used, we\\ndequantize the tensor to BFloat16, and then perform a matrix multiplication in 16-bit.\\nWe now discuss the components of QL ORA followed by a formal definition of QL ORA.\\n4-bit NormalFloat Quantization The NormalFloat (NF) data type builds on Quantile Quantization\\n[15,46] which is an information-theoretically optimal data type that ensures each quantization bin has\\nan equal number of values assigned from the input tensor. Quantile quantization works by estimating\\nthe quantile of the input tensor through the empirical cumulative distribution function.\\nThe main limitation of quantile quantization is that the process of quantile estimation is expensive.\\nTherefore fast quantile approximation algorithms, such as SRAM quantiles [ 15], are used to estimate\\nthem. Due to the approximate nature of these quantile estimation algorithms, the data type has large\\nquantization errors for outliers, which are often the most important values.\\nExpensive quantile estimates and approximation errors can be avoided when input tensors come from\\na distribution fixed up to a quantization constant. In such cases, input tensors have the same quantiles\\nmaking exact quantile estimation computationally feasible.\\nSince pretrained neural network weights usually have a zero-centered normal distribution with\\nstandard deviation σ(see Appendix I), we can transform all weights to a single fixed distribution by\\nscaling σsuch that the distribution fits exactly into the range of our data type. For our data type, we\\nset the arbitrary range [−1,1]. As such, both the quantiles for the data type and the neural network\\nweights need to be normalized into this range.\\nThe information theoretically optimal data type for zero-mean normal distributions with arbitrary\\nstandard deviations σin the range [−1,1]is computed as follows: (1) estimate the 2k+ 1quantiles\\nof a theoretical N(0,1)distribution to obtain a k-bit quantile quantization data type for normal distri-\\nbutions, (2) take this data type and normalize its values into the [−1,1]range, (3) quantize an input\\nweight tensor by normalizing it into the [−1,1]range through absolute maximum rescaling.\\nOnce the weight range and data type range match, we can quantize as usual. Step (3) is equivalent to\\nrescaling the standard deviation of the weight tensor to match the standard deviation of the k-bit data\\ntype. More formally, we estimate the 2kvalues qiof the data type as follows:\\nqi=1\\n2\\x12\\nQX\\x12i\\n2k+ 1\\x13\\n+QX\\x12i+ 1\\n2k+ 1\\x13\\x13\\n, (4)\\nwhere QX(·)is the quantile function of the standard normal distribution N(0,1). A problem for\\na symmetric k-bit quantization is that this approach does not have an exact representation of zero,\\nwhich is an important property to quantize padding and other zero-valued elements with no error. To\\nensure a discrete zeropoint of 0and to use all 2kbits for a k-bit datatype, we create an asymmetric\\ndata type by estimating the quantiles qiof two ranges qi:2k−1for the negative part and 2k−1+ 1\\nfor the positive part and then we unify these sets of qiand remove one of the two zeros that occurs\\nin both sets. We term the resulting data type that has equal expected number of values in each\\nquantization bin k-bit NormalFloat (NFk), since the data type is information-theoretically optimal for\\nzero-centered normally distributed data. A step-by-step visualization of how to construct the NF4\\ndata type and its exact values can be found in Appendix H.\\nDouble Quantization We introduce Double Quantization (DQ), the process of quantizing the\\nquantization constants for additional memory savings. While a small blocksize is required for precise\\n4-bit quantization [ 13], it also has a considerable memory overhead. For example, using 32-bit\\nconstants and a blocksize of 64 for W, quantization constants add 32/64 = 0 .5bits per parameter on\\naverage. Double Quantization helps reduce the memory footprint of quantization constants.\\nMore specifically, Double Quantization treats quantization constants cFP32\\n2of the first quantization\\nas inputs to a second quantization. This second step yields the quantized quantization constants\\ncFP8\\n2and the second level of quantization constants cFP32\\n1. We use 8-bit Floats with a blocksize of\\n256 for the second quantization as no performance degradation is observed for 8-bit quantization,\\nin line with results from Dettmers and Zettlemoyer [13]. Since the cFP32\\n2are positive, we subtract\\nthe mean from c2before quantization to center the values around zero and make use of symmetric\\n4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f6a0b2ab-d4cd-435a-b9e2-0a29d58ef62d', embedding=None, metadata={'page_label': '5', 'file_name': 'Q-Lora.pdf', 'file_path': 'D:\\\\Courses\\\\RAG Course\\\\Q-Lora.pdf', 'file_type': 'application/pdf', 'file_size': 1110975, 'creation_date': '2024-07-18', 'last_modified_date': '2024-07-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='quantization. On average, for a blocksize of 64, this quantization reduces the memory footprint per\\nparameter from 32/64 = 0 .5bits, to 8/64 + 32 /(64·256) = 0 .127bits, a reduction of 0.373 bits\\nper parameter.\\nPaged Optimizers use the NVIDIA unified memory3feature which does automatic page-to-page\\ntransfers between the CPU and GPU for error-free GPU processing in the scenario where the GPU\\noccasionally runs out-of-memory. The feature works like regular memory paging between CPU RAM\\nand the disk. We use this feature to allocate paged memory for the optimizer states which are then\\nautomatically evicted to CPU RAM when the GPU runs out-of-memory and paged back into GPU\\nmemory when the memory is needed in the optimizer update step.\\nQL ORA.Using the components described above, we define QLORAfor a single linear layer in\\nthe quantized base model with a single LoRA adapter as follows:\\nYBF16=XBF16doubleDequant (cFP32\\n1, ck-bit\\n2,WNF4) +XBF16LBF16\\n1LBF16\\n2, (5)\\ndoubleDequant (cFP32\\n1, ck-bit\\n2,Wk-bit) =dequant (dequant (cFP32\\n1, ck-bit\\n2),W4bit) =WBF16,(6)\\nWe use NF4 for Wand FP8 for c2. We use a blocksize of 64 for Wfor higher quantization precision\\nand a blocksize of 256 for c2to conserve memory.\\nFor parameter updates only the gradient with respect to the error for the adapters weights∂E\\n∂Liare\\nneeded, and not for 4-bit weights∂E\\n∂W. However, the calculation of∂E\\n∂Lientails the calculation of∂X\\n∂W\\nwhich proceeds via equation (5) with dequantization from storage WNF4to computation data type\\nWBF16to calculate the derivative∂X\\n∂Win BFloat16 precision.\\nTo summarize, QLORAhas one storage data type (usually 4-bit NormalFloat) and a computation\\ndata type (16-bit BrainFloat). We dequantize the storage data type to the computation data type\\nto perform the forward and backward pass, but we only compute weight gradients for the LoRA\\nparameters which use 16-bit BrainFloat.\\n4 QLoRA vs. Standard Finetuning\\n1010\\n1011\\nT otal model bits\\n0.60\\n0.61\\n0.62\\n0.63\\n0.64\\n0.65\\n0.66\\n0.67Mean zeroshot accuracy\\n4-bit LLaMA\\nFloat\\nNFloat\\nNFloat + DQData type\\nFigure 2: Mean zero-shot accuracy over Wino-\\ngrande, HellaSwag, PiQA, Arc-Easy, and Arc-\\nChallenge using LLaMA models with different 4-bit\\ndata types. The NormalFloat data type significantly\\nimproves the bit-for-bit accuracy gains compared\\nto regular 4-bit Floats. While Double Quantization\\n(DQ) only leads to minor gains, it allows for a more\\nfine-grained control over the memory footprint to fit\\nmodels of certain size (33B/65B) into certain GPUs\\n(24/48GB).We have discussed how QLoRA works and how\\nit can significantly reduce the required memory\\nfor finetuning models. The main question now is\\nwhether QLoRA can perform as well as full-model\\nfinetuning. Furthermore, we want to analyze the\\ncomponents of QLoRA including the impact of\\nNormalFloat4 over standard Float4. The follow-\\ning sections will discuss the experiments that aimed\\nat answering these questions.\\nExperimental setup. We consider three architec-\\ntures (encoder, encoder-decoder, and decoder only)\\nand compare QLoRA with 16-bit adapter-finetuning\\nand with full-finetuning for models up to 3B. Our\\nevaluations include GLUE [ 59] with RoBERTa-\\nlarge [ 38], Super-NaturalInstructionss [ 62] with\\nT5 [50], and 5-shot MMLU [ 24] after finetuning\\nLLaMA on Flan v2 [39] and Alpaca [56].\\nTo additionally study the advantages of NF4\\nover other 4-bit data types, we use the setup\\nof Dettmers and Zettlemoyer [13] and measure\\npost-quantization zero-shot accuracy and perplexity\\nacross different models (OPT [ 73], LLaMA [ 58],\\nBLOOM [ 53], Pythia [ 7]) for model sizes 125m - 13B. We provide more details in the results section\\nfor each particular setup to make the results more readable. Full details in Appendix C.\\nWhile paged optimizers are critical to do 33B/65B QLORAtuning on a single 24/48GB GPU, we do\\nnot provide hard measurements for Paged Optimizers since the paging only occurs when processing\\n3https://docs.nvidia.com/cuda/cuda-c-programming-guide\\n5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='575859c6-9a85-48f3-b0b8-322b6ab3ea1c', embedding=None, metadata={'page_label': '6', 'file_name': 'Q-Lora.pdf', 'file_path': 'D:\\\\Courses\\\\RAG Course\\\\Q-Lora.pdf', 'file_type': 'application/pdf', 'file_size': 1110975, 'creation_date': '2024-07-18', 'last_modified_date': '2024-07-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 3: Mean 5-shot MMLU test accuracy for LLaMA 7-65B models finetuned with adapters on Alpaca and\\nFLAN v2 for different data types. Overall, NF4 with double quantization (DQ) matches BFloat16 performance,\\nwhile FP4 is consistently one percentage point behind both.\\nMean 5-shot MMLU Accuracy\\nLLaMA Size 7B 13B 33B 65B Mean\\nDataset Alpaca FLAN v2 Alpaca FLAN v2 Alpaca FLAN v2 Alpaca FLAN v2\\nBFloat16 38.4 45.6 47.2 50.6 57.7 60.5 61.8 62.5 53.0\\nFloat4 37.2 44.0 47.3 50.0 55.9 58.5 61.3 63.3 52.2\\nNFloat4 + DQ 39.0 44.5 47.5 50.7 57.3 59.2 61.8 63.9 53.1\\nmini-batches with long sequence lengths, which is rare. We do, however, perform an analysis of the\\nruntime of paged optimizers for 65B models on 48GB GPUs and find that with a batch size of 16,\\npaged optimizers provide the same training speed as regular optimizers. Future work should measure\\nand characterize under what circumstances slow-downs occur from the paging process.\\nTable 2: Pile Common Crawl mean\\nperplexity for different data types\\nfor 125M to 13B OPT, BLOOM,\\nLLaMA, and Pythia models.\\nData type Mean PPL\\nInt4 34.34\\nFloat4 (E2M1) 31.07\\nFloat4 (E3M0) 29.48\\nNFloat4 + DQ 27.414-bit NormalFloat yields better performance than 4-bit Float-\\ning Point While the 4-bit NormalFloat (NF4) data type is\\ninformation-theoretically optimal, it still needs to be determined\\nif this property translates to empirical advantages. We follow\\nthe setup from Dettmers and Zettlemoyer [13] where quantized\\nLLMs (OPT [ 73], BLOOM [ 53], Pythia [ 7], LLaMA) of different\\nsizes (125M to 65B) with different data types are evaluated on\\nlanguage modeling and a set of zero-shot tasks. In Figure 7 and\\nTable 2 we see that NF4 improves performance significantly over\\nFP4 and Int4 and that double quantization reduces the memory\\nfootprint without degrading performance.\\nk-bit QL ORAmatches 16-bit full finetuning and 16-bit LoRA performance Recent findings\\nhave established that 4-bit quantization for inference is possible, but leads to performance degradation\\nrelative to 16-bit [ 13,18]. This raises the crucial question of whether the lost performance can be\\nrecovered by conducting 4-bit adapter finetuning. We test this for two setups.\\nThe first setup test if QLoRA can replicate 16-bit full finetuning performance for T5 and RoBERTa\\nmodel finetuning. These results are detailed in the Appendix C.1. For our second setup, since full\\nfinetuning models at and beyond 11B parameters requires more than one server of high memory\\nGPUs, we continue to test whether 4-bit QLORAcan match 16-bit LoRA at the 7B to 65B parameter\\nscales. To this end, we finetune LLaMA 7B through 65B on two instruction following datasets,\\nAlpaca and FLAN v2, and evaluate on the MMLU benchmark via 5-shot accuracy. Results are shown\\nin Table 3 where we see that NF4 with double quantization fully recovers the 16-bit LoRA MMLU\\nperformance. In addition, we also note that QLORAwith FP4 lags behind the 16-bit brain float\\nLoRA baseline by about 1 percentage point. This corroborates both our findings that (1) QLORA\\nwith NF4 replicates both 16-bit full finetuning and 16-bit LoRA finetuning performance, and (2) NF4\\nis superior to FP4 in terms of quantization precision.\\nSummary Our results consistently show that 4-bit QLORAwith NF4 data type matches 16-\\nbit full finetuning and 16-bit LoRA finetuning performance on academic benchmarks with well-\\nestablished evaluation setups. We have also shown that NF4 is more effective than FP4 and that\\ndouble quantization does not degrade performance. Combined, this forms compelling evidence that\\n4-bit QL ORA tuning reliably yields results matching 16-bit methods.\\nIn line with previous work on quantization [ 13], our MMLU and Elo results indicate that with a given\\nfinetuning and inference resource budget it is beneficial to increase the number of parameters in the\\nbase model while decreasing their precision. This highlights the importance of efficiency benefits\\nfrom QLORA. Since we did not observe performance degradation compared to full-finetuning in\\nour experiments with 4-bit finetuning, this raises the question of where the performance-precision\\ntrade-off exactly lies for QLoRA tuning, which we leave to future work to explore.\\nWe proceed to investigate instruction tuning at scales that would be impossible to explore with full\\n16-bit finetuning on academic research hardware.\\n6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bf626118-b2d4-4d89-bcaf-c6728ee078f3', embedding=None, metadata={'page_label': '7', 'file_name': 'Q-Lora.pdf', 'file_path': 'D:\\\\Courses\\\\RAG Course\\\\Q-Lora.pdf', 'file_type': 'application/pdf', 'file_size': 1110975, 'creation_date': '2024-07-18', 'last_modified_date': '2024-07-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5 Pushing the Chatbot State-of-the-art with QLoRA\\nHaving established that 4-bit QLORAmatches 16-bit performance across scales, tasks, and datasets\\nwe conduct an in-depth study of instruction finetuning up to the largest open-source language models\\navailable for research. To assess the performance of instruction finetuning these models, we evaluate\\non a challenging Natural Language Understanding benchmark (MMLU) and develop new methods for\\nreal-world chatbot performance evaluation. For a more qualitative analysis, see Appendix F.\\n5.1 Experimental setup\\nWe now describe an overview of the experimental setup with full details in Appendix D.\\nData As, to our knowledge, there is no comprehensive study of instruction-following datasets, we\\nselect eight recent datasets. We include datasets obtained through crowd-sourcing (OASST1 [ 31],\\nHH-RLHF [ 4]), distillation from instruction-tuned models (Alpaca [ 56], self-instruct [ 60], unnatural-\\ninstructions [ 26]), corpora aggregations (FLAN v2 [ 12]), as well as hybrids (Chip2 [ 32], Long-\\nform [30]). These datasets cover different languages, data sizes, and licenses.\\nTraining Setup To avoid confounding effects from different training objectives, we perform\\nQLoRA finetuning with cross-entropy loss (supervised learning) without reinforcement learning,\\neven for datasets that include human judgments of different responses. For datasets that have a clear\\ndistinction between instruction and response, we finetune only on the response (see ablations in\\nAppendix D). For OASST1 and HH-RLHF, multiple responses are available. We then select the\\ntop response at every level of the conversation tree and finetune on the full selected conversation,\\nincluding the instructions. In all of our experiments, we use NF4 QLORAwith double quantization\\nand paged optimizers to prevent memory spikes during gradient checkpointing. We do a small\\nhyperparameter searches for the 13B and 33B LLaMA models and we find that all hyperparameter\\nsettings found at 7B generalize (including number of epochs) except learning rate and batch size. We\\nhalve the learning rate for 33B and 65B while doubling the batch size.\\nBaselines We compare our models to both research (Vicuna [ 10] and Open Assistant [ 31]) and\\ncommercial (GPT-4 [ 42], GPT-3.5-turbo and Bard) chatbot systems. The Open Assistant model\\nis a LLaMA 33B model finetuned with Reinforcement Learning from Human Feedback (RLHF)\\non the OASST1 dataset. Vicuna does full fine-tuning of LLaMA 13B on proprietary user-shared\\nconversations from ShareGPT and is thus the result of distillation from OpenAI GPT models.\\nEvaluation setup Chatbot evaluation is not straightforward since each prompt has many high-\\nquality responses which are challenging to rank. We therefore follow a comprehensive approach,\\nwhich involves (a) standard benchmarks that measures general language understanding performance\\n(MMLU [ 24]), (b) both automatic and human evaluation that measures how many responses from\\nchatbot A are better compared to chatbot B, (c) a round-robin tournament-style evaluation where\\nchatbots compete against each other in games where chatbot performance is measured as ELO. We\\nprovide a detailed discussion of these evaluation setups in the Appendix E.\\n5.2 Guanaco: QL ORA trained on OASST1 is a State-of-the-art Chatbot\\nBased on our automated and human evaluations, we find that the top QLORAtuned model, Guanaco\\n65B, which we finetune on a variant of OASST1, is the best-performing open-source chatbot model\\nand offers performance competitive to ChatGPT. When compared to GPT-4, Guanaco 65B and 33B\\nhave an expected win probability of 30%, based on Elo rating from human annotators system-level\\npairwise comparisons on the Vicuna benchmark - the highest reported to date.\\nThe Vicuna benchmark [ 10] results relative to ChatGPT are shown in Table 4. We find that Guanaco\\n65B is the best-performing model after GPT-4, achieving 99.3% performance relative to ChatGPT.\\nGuanaco 33B has more parameters than the Vicuna 13B model, but uses only 4-bit precision for its\\nweights and is thus much more memory efficient at 21 GB vs 26 GB, providing a three percentage\\npoints of improvement over Vicuna 13B. Furthermore, Guanaco 7B easily fits on modern phones at a\\n5 GB footprint while still scoring nearly 20 percentage points higher than Alpaca 13B.\\nHowever, Table 4 also has very wide confidence intervals, with many models overlapping in per-\\nformance. We hypothesize that this uncertainty comes from the lack of clear specification of scale,\\ne.g., it is unclear what 8 on a 10 point scale means across different scenarios. As such, we instead\\nrecommend using the Elo ranking method [ 16], based on pairwise judgments from human annotators\\nand GPT-4 to avoid the problem of grounding an absolute scale.\\n7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e6bf0ef1-969b-4606-b7b0-532af78ca2bd', embedding=None, metadata={'page_label': '8', 'file_name': 'Q-Lora.pdf', 'file_path': 'D:\\\\Courses\\\\RAG Course\\\\Q-Lora.pdf', 'file_type': 'application/pdf', 'file_size': 1110975, 'creation_date': '2024-07-18', 'last_modified_date': '2024-07-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 4: Zero-shot Vicuna benchmark scores as a percentage of the score obtained by ChatGPT evaluated by\\nGPT-4. We see that OASST1 models perform close to ChatGPT despite being trained on a very small dataset\\nand having a fraction of the memory requirement of baseline models.\\nModel / Dataset Params Model bits Memory ChatGPT vs Sys Sys vs ChatGPT Mean 95% CI\\nGPT-4 - - - 119.4% 110.1% 114.5 % 2.6%\\nBard - - - 93.2% 96.4% 94.8% 4.1%\\nGuanaco 65B 4-bit 41 GB 96.7% 101.9% 99.3% 4.4%\\nAlpaca 65B 4-bit 41 GB 63.0% 77.9% 70.7% 4.3%\\nFLAN v2 65B 4-bit 41 GB 37.0% 59.6% 48.4% 4.6%\\nGuanaco 33B 4-bit 21 GB 96.5% 99.2% 97.8% 4.4%\\nOpen Assistant 33B 16-bit 66 GB 73.4% 85.7% 78.1% 5.3%\\nAlpaca 33B 4-bit 21 GB 67.2% 79.7% 73.6% 4.2%\\nFLAN v2 33B 4-bit 21 GB 26.3% 49.7% 38.0% 3.9%\\nVicuna 13B 16-bit 26 GB 91.2% 98.7% 94.9% 4.5%\\nGuanaco 13B 4-bit 10 GB 87.3% 93.4% 90.4% 5.2%\\nAlpaca 13B 4-bit 10 GB 63.8% 76.7% 69.4% 4.2%\\nHH-RLHF 13B 4-bit 10 GB 55.5% 69.1% 62.5% 4.7%\\nUnnatural Instr. 13B 4-bit 10 GB 50.6% 69.8% 60.5% 4.2%\\nChip2 13B 4-bit 10 GB 49.2% 69.3% 59.5% 4.7%\\nLongform 13B 4-bit 10 GB 44.9% 62.0% 53.6% 5.2%\\nSelf-Instruct 13B 4-bit 10 GB 38.0% 60.5% 49.1% 4.6%\\nFLAN v2 13B 4-bit 10 GB 32.4% 61.2% 47.0% 3.6%\\nGuanaco 7B 4-bit 5 GB 84.1% 89.8% 87.0% 5.4%\\nAlpaca 7B 4-bit 5 GB 57.3% 71.2% 64.4% 5.0%\\nFLAN v2 7B 4-bit 5 GB 33.3% 56.1% 44.8% 4.0%\\nTable 5: MMLU 5-shot test results for different\\nsizes of LLaMA finetuned on the corresponding\\ndatasets using QLoRA.\\nDataset 7B 13B 33B 65B\\nLLaMA no tuning 35.1 46.9 57.8 63.4\\nSelf-Instruct 36.4 33.3 53.0 56.7\\nLongform 32.1 43.2 56.6 59.7\\nChip2 34.5 41.6 53.6 59.8\\nHH-RLHF 34.9 44.6 55.8 60.1\\nUnnatural Instruct 41.9 48.1 57.3 61.3\\nGuanaco (OASST1) 36.6 46.4 57.0 62.2\\nAlpaca 38.8 47.8 57.3 62.5\\nFLAN v2 44.5 51.4 59.2 63.9Elo ratings of the most competitive models can be seen\\nin Table 1. We note that human and GPT-4 ranking\\nof models on the Vicuna benchmark disagree partially,\\nparticularly for Guanaco 7B, but are consistent for most\\nmodels with a Kendall Tau of τ= 0.43and Spearman\\nrank correlation of r= 0.55at the system level. At\\nthe example level, the agreement between GPT-4 and\\nhuman annotators’ majority vote is weaker with Fleiss\\nκ= 0.25. Overall, this shows a moderate agreement\\nbetween system-level judgments by GPT-4 and human\\nannotators, and thus that model-based evaluation rep-\\nresents a somewhat reliable alternative to human evalu-\\nation. See Section E for further considerations.\\nElo rankings in Table 6 indicate that Guanaco 33B and 65B models outperform all models besides\\nGPT-4 on the Vicuna and OA benchmarks and that they perform comparably to ChatGPT in line\\nwith Table 4. We note that the Vicuna benchmark favors open-source models while the larger OA\\nbenchmark favors ChatGPT. Furthermore, we can see from Tables 5 and 4 that the suitability of a\\nfinetuning dataset is a determining factor in performance. Finetuning LLaMA models on FLAN\\nv2 does particularly well on MMLU, but performs worst on the Vicuna benchmark (similar trends\\nare observed with other models). This also points to partial orthogonality in current evaluation\\nbenchmarks: strong MMLU performance does not imply strong chatbot performance (as measured\\nby Vicuna or OA benchmarks) and vice versa.\\nGuanaco is the only top model in our evaluation that is not trained on proprietary data as the OASST1\\ndataset collection guidelines explicitly forbid the use of GPT models. The next best model trained\\non only open-source data is the Anthropic HH-RLHF model, which scores 30 percentage points\\nlower than Guanaco on the Vicuna benchmark (see Table 4). Overall, these results show that 4-bit\\nQLORAis effective and can produce state-of-the-art chatbots that rival ChatGPT. Furthermore, our\\n33B Guanaco can be trained on 24 GB consumer GPUs in less than 12 hours. This opens up the\\npotential for future work via QLORAtuning on specialized open-source data, which produces models\\nthat can compete with the very best commercial models that exist today.\\n8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a0680809-4539-47e6-9734-c7e43344e60c', embedding=None, metadata={'page_label': '9', 'file_name': 'Q-Lora.pdf', 'file_path': 'D:\\\\Courses\\\\RAG Course\\\\Q-Lora.pdf', 'file_type': 'application/pdf', 'file_size': 1110975, 'creation_date': '2024-07-18', 'last_modified_date': '2024-07-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 6: Elo rating for a tournament between models where models compete to generate the best response\\nfor a prompt, judged by human raters or GPT-4. Overall, Guanaco 65B and 33B tend to be preferred to\\nChatGPT-3.5 on the benchmarks studied. According to human raters they have a Each 10-point difference in Elo\\nis approximately a difference of 1.5% in win-rate.\\nBenchmark Vicuna Vicuna Open Assistant\\n# Prompts 80 80 953\\nJudge Human raters GPT-4 GPT-4 Median Rank\\nModel Elo Rank Elo Rank Elo Rank\\nGPT-4 1176 1 1348 1 1294 1 1\\nGuanaco-65B 1023 2 1022 2 1008 3 2\\nGuanaco-33B 1009 4 992 3 1002 4 4\\nChatGPT-3.5 Turbo 916 7 966 5 1015 2 5\\nVicuna-13B 984 5 974 4 936 5 5\\nGuanaco-13B 975 6 913 6 885 6 6\\nGuanaco-7B 1010 3 879 8 860 7 7\\nBard 909 8 902 7 - - 8\\n6 Related Work\\nQuantization of Large Language Models Quantization of LLMs has largely focused on quanti-\\nzation for inference time. Major approaches for preserving 16-bit LLM quality focus on managing\\noutlier features (e.g., SmoothQuant [ 67] and LLM.int8() [ 14]) while others use more sophisticated\\ngrouping methods [ 44,70]. Lossy quantization approaches study the trade-offs for regular round-\\ning [ 13,72,48] or how to optimize rounding decisions to improve quantization precision [ 18].\\nBesides our work, SwitchBack layers [ 66] is the only work that studies backpropagation through\\nquantized weights at a scale beyond 1B parameters.\\nFinetuning with Adapters While we use Low-rank Adapters [ 28] (LoRA), many other Parameter\\nEfficient FineTuning (PEFT) methods have been proposed such as prompt tuning [ 49,33,34], tuning\\nthe embedding layer inputs [ 1], tuning hidden states (IA3) [37], adding full layers [ 27], tuning\\nbiases [ 71], learning a mask over weights based on Fisher information [ 55], and a combination of\\napproaches [ 23]. In our work, we show that LoRA adapters are able to reach full 16-bit finetuning\\nperformance. We leave it to future work to explore the tradeoffs of other PEFT approaches.\\nInstruction Finetuning To help a pretrained LLM follow the instructions provided in a prompt,\\ninstruction finetuning uses input-output pairs of various data sources to finetune a pretrained LLM\\nto generate the output given the input as a prompt. Approaches and datasets include MetaICL [ 40],\\nMetaTuning [ 74], InstructGPT [ 43], FLAN [ 63,12], PromptSource [ 3], Super-NaturalInstructions [ 62,\\n51], Self-instruct [ 60], UnnaturalInstructions [ 26], OPT-IML [ 29], UnifiedSKG[ 68], OIG/Chip2 [ 32],\\nAlpaca [56], Vicuna [10], Koala [20], and Self-instruct-GPT-4 [45].\\nChatbots Many instruction following models are structured as dialogue-based chatbots, often using\\nReinforcement Learning from Human Feedback (RLHF) [ 11] or generating data from an existing\\nmodel to train with AI model feedback (RLAIF) [ 5]. Approaches and datasets include Anthropic-\\nHH [ 2,4], Open Assistant [ 31], LaMDA [ 57], and Sparrow [ 21]. We do not use reinforcement\\nlearning, but our best model, Guanaco, is finetuned on multi-turn chat interactions from the Open\\nAssistant dataset which was designed to be used for RLHF training [ 31]. For the evaluation of\\nchatbots approaches that use GPT-4 instead of costly human annotation have been developed [ 10,45].\\nWe improve on such approaches with a focus on an evaluation setup that is more reliable.\\n7 Limitations and Discussion\\nWe have shown evidence that our method, QLORA, can replicate 16-bit full finetuning performance\\nwith a 4-bit base model and Low-rank Adapters. However, due to the immense resource cost, we were\\nnot establish that QL ORA can match 16-bit finetuning performance at 33B and 65B scales.\\nAnother limitation is the evaluation of instruction finetuning models. While we provide evaluations\\non MMLU, the Vicuna/OA benchmark, we did not evaluate on BigBench, RAFT, and HELM, and it\\n9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f29adc1f-bfe5-4df8-a913-c67bf62ff4b7', embedding=None, metadata={'page_label': '10', 'file_name': 'Q-Lora.pdf', 'file_path': 'D:\\\\Courses\\\\RAG Course\\\\Q-Lora.pdf', 'file_type': 'application/pdf', 'file_size': 1110975, 'creation_date': '2024-07-18', 'last_modified_date': '2024-07-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='is not ensured that our evaluations generalize to these benchmarks. On the other hand, we perform a\\nvery broad study on MMLU and develop new methods for evaluating chatbots.\\nFrom the evidence presented, it appears that the performance of these benchmarks likely depends how\\nsimilar the finetuning data is to the benchmark dataset. For example, FLAN v2 is similar to MMLU,\\nbut dissimilar to the Vicuna benchmark and vice versa for the Chip2 dataset. This highlights that not\\nonly better benchmarks and evaluation is needed, but that one needs to be careful about what one is\\nevaluating in the first place. Do we want to create models that do well on classroom highschool and\\ncolleague knowledge or do we want to do well on chatbot conversation ability? Maybe something\\nelse? Because it is always easier to evaluate on an existing benchmark compared to creating a new\\none, certain benchmarks can steer the community towards a certain direction. We should ensure as a\\ncommunity that the benchmarks measure what we care about.\\nAn additional limitation is that we did not evaluate different bit-precisions or different adapter\\nmethods. Besides LoRA, there is also a wide variety Parameter Efficient FineTuning (PEFT) methods\\nthat have been shown to work well. However, it is unclear if these methods scale to large models.\\nWe used LoRA as many results established its robustness but other adapters might yield better\\nperformance. Since finetuning after quantization seems to recover most of the information that is lost\\nduring quantization this might enable much more aggressive quantization. For example, 3-bit GPTQ\\nquantization might also yield 16-bit full finetuning performance after finetuning.\\n8 Broader Impacts\\nOur QLORAfinetuning method is the first method that enables the finetuning of 33B parameter\\nmodels on a single consumer GPU and 65B parameter models on a single professional GPU, while\\nnot degrading performance relative to a full finetuning baseline. We have demonstrated that our\\nbest 33B model trained on the Open Assistant dataset can rival ChatGPT on the Vicuna benchmark.\\nSince instruction finetuning is an essential tool to transform raw pretrained LLMs into ChatGPT-like\\nchatbots, we believe that our method will make finetuning widespread and common in particular for\\nthe researchers that have the least resources – a big win for the accessibility of state of the art NLP\\ntechnology. QLORAcan be seen as an equalizing factor that helps to close the resource gap between\\nlarge corporations and small teams with consumer GPUs.\\nAnother potential source of impact is deployment to mobile phones and other low resources devices.\\nWhile 7B models were shown to be able to be run on phones before, QLORAis the first method\\nthat would enable the finetuning of such models. We estimate that with an iPhone 12 Plus, QLORA\\ncan finetune 3 million tokens per night while the phone is charging. QLORAcan help enable\\nprivacy-preserving usage of LLMs, where users can own and manage their own data and models,\\nwhile simultaneously making LLMs easier to deploy.\\nFurthermore, because of the increased inference efficiency of 4-bit models, if QLoRA models are\\ndeployed they reduce the environmental impact that LLMs have when deployed for personal use.\\nWe estimate that if 50% of deployments are personal and 50% are company deployments, QLoRA\\ndeployments could reduce the overall carbon footprint by 72% (see Appendix B).\\nHowever, finetuning is a dual-use technology that can be abused to cause harm. Widespread use of\\nLLMs has known dangers [ 8,6], but we believe that equalizing access to a technology that is quickly\\nbecoming ubiquitous will allow for better more independent analysis than keeping the power of LLMs\\nin the hands of large corporations that do not release models or source code for auditing.\\nAll in all, we believe that QLORAwill have a broadly positive impact making the finetuning of high\\nquality LLMs much more widely and easily accessible.\\nAcknowledgements\\nWe thank Aditya Kusupati, Ofir Press, Ashish Sharma, Margaret Li, Raphael Olivier, Zihao Ye, and\\nEvangelia Spiliopoulou for their valuable feedback. We thank Norah Altriri for providing us with an\\nintuitive NF4 visualization. We thank Felix Petersen for a discussion on distributional quantization and\\nits relationship to quantile quantization. Our research was facilitated by the advanced computational,\\nstorage, and networking infrastructure of the Hyak supercomputer system at the University of\\nWashington. We thank the Hyak team for ensuring a smooth operation. We thank the beta testers of\\nthe bitsandbytes library, in particular Alex Birch and Alyssa Vance. We thank Younes Belkada for\\nhelp with the integration of our software into the Hugging Face transformers stack.\\n10', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='829ff589-1d41-4cbf-9e29-2b937c984da3', embedding=None, metadata={'page_label': '11', 'file_name': 'Q-Lora.pdf', 'file_path': 'D:\\\\Courses\\\\RAG Course\\\\Q-Lora.pdf', 'file_type': 'application/pdf', 'file_size': 1110975, 'creation_date': '2024-07-18', 'last_modified_date': '2024-07-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='References\\n[1]S. An, Y . Li, Z. Lin, Q. Liu, B. Chen, Q. Fu, W. Chen, N. Zheng, and J.-G. Lou. Input-tuning:\\nAdapting unfamiliar inputs to frozen pretrained models. arXiv preprint arXiv:2203.03131 ,\\n2022.\\n[2]A. Askell, Y . Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann,\\nN. DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint\\narXiv:2112.00861 , 2021.\\n[3]S. H. Bach, V . Sanh, Z.-X. Yong, A. Webson, C. Raffel, N. V . Nayak, A. Sharma, T. Kim, M. S.\\nBari, T. Fevry, et al. Promptsource: An integrated development environment and repository for\\nnatural language prompts. arXiv preprint arXiv:2202.01279 , 2022.\\n[4]Y . Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli,\\nT. Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from\\nhuman feedback. arXiv preprint arXiv:2204.05862 , 2022.\\n[5]Y . Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirho-\\nseini, C. McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint\\narXiv:2212.08073 , 2022.\\n[6]E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the dangers of stochastic\\nparrots: Can language models be too big? In Proceedings of the 2021 ACM conference on\\nfairness, accountability, and transparency , pages 610–623, 2021.\\n[7]S. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley, K. O’Brien, E. Hallahan, M. A. Khan,\\nS. Purohit, U. S. Prashanth, E. Raff, et al. Pythia: A suite for analyzing large language models\\nacross training and scaling. arXiv preprint arXiv:2304.01373 , 2023.\\n[8]R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein,\\nJ. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models.\\narXiv preprint arXiv:2108.07258 , 2021.\\n[9]T. Chen, B. Xu, C. Zhang, and C. Guestrin. Training deep nets with sublinear memory cost.\\narXiv preprint arXiv:1604.06174 , 2016.\\n[10] W.-L. Chiang, Z. Li, Z. Lin, Y . Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y . Zhuang, J. E.\\nGonzalez, I. Stoica, and E. P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%*\\nchatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/ .\\n[11] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement\\nlearning from human preferences. Advances in neural information processing systems , 30,\\n2017.\\n[12] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y . Tay, W. Fedus, E. Li, X. Wang, M. De-\\nhghani, S. Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint\\narXiv:2210.11416 , 2022.\\n[13] T. Dettmers and L. Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. arXiv\\npreprint arXiv:2212.09720 , 2022.\\n[14] T. Dettmers, M. Lewis, Y . Belkada, and L. Zettlemoyer. LLM.int8(): 8-bit matrix multiplication\\nfor transformers at scale. Advances in Neural Information Processing Systems 35: Annual\\nConference on Neural Information Processing Systems 2022, NeurIPS 2022 , 2022.\\n[15] T. Dettmers, M. Lewis, S. Shleifer, and L. Zettlemoyer. 8-bit optimizers via block-wise\\nquantization. 9th International Conference on Learning Representations, ICLR , 2022.\\n[16] A. E. Elo. The proposed uscf rating system. its development, theory, and applications. Chess\\nLife, 22(8):242–247, 1967.\\n[17] A. E. Elo. The rating of chessplayers, past and present . Arco Pub., 1978.\\n11', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f132ca28-dd46-4755-8d16-28a5bb6c1ef3', embedding=None, metadata={'page_label': '12', 'file_name': 'Q-Lora.pdf', 'file_path': 'D:\\\\Courses\\\\RAG Course\\\\Q-Lora.pdf', 'file_type': 'application/pdf', 'file_size': 1110975, 'creation_date': '2024-07-18', 'last_modified_date': '2024-07-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='[18] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. Gptq: Accurate post-training quantization\\nfor generative pre-trained transformers. arXiv preprint arXiv:2210.17323 , 2022.\\n[19] J. Fu, S.-K. Ng, Z. Jiang, and P. Liu. Gptscore: Evaluate as you desire. arXiv preprint\\narXiv:2302.04166 , 2023.\\n[20] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine, and D. Song. Koala: A\\ndialogue model for academic research. Blog post, April 2023. URL https://bair.berkeley.\\nedu/blog/2023/04/03/koala/ .\\n[21] A. Glaese, N. McAleese, M. Tr˛ ebacz, J. Aslanides, V . Firoiu, T. Ewalds, M. Rauh, L. Weidinger,\\nM. Chadwick, P. Thacker, et al. Improving alignment of dialogue agents via targeted human\\njudgements. arXiv preprint arXiv:2209.14375 , 2022.\\n[22] S. Gururangan, S. Swayamdipta, O. Levy, R. Schwartz, S. R. Bowman, and N. A. Smith.\\nAnnotation artifacts in natural language inference data. arXiv preprint arXiv:1803.02324 , 2018.\\n[23] J. Henderson, S. Ruder, et al. Compacter: Efficient low-rank hypercomplex adapter layers. In\\nAdvances in Neural Information Processing Systems , 2021.\\n[24] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Mea-\\nsuring massive multitask language understanding. In International Conference on Learning\\nRepresentations , 2020.\\n[25] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y . Choi. The curious case of neural text\\ndegeneration. In International Conference on Learning Representations , 2020.\\n[26] O. Honovich, T. Scialom, O. Levy, and T. Schick. Unnatural instructions: Tuning language\\nmodels with (almost) no human labor. arXiv preprint arXiv:2212.09689 , 2022.\\n[27] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. At-\\ntariyan, and S. Gelly. Parameter-efficient transfer learning for nlp. In International Conference\\non Machine Learning , pages 2790–2799. PMLR, 2019.\\n[28] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang, and W. Chen. Lora:\\nLow-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 , 2021.\\n[29] S. Iyer, X. V . Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster, T. Wang, Q. Liu, P. S.\\nKoura, et al. Opt-iml: Scaling language model instruction meta learning through the lens of\\ngeneralization. arXiv preprint arXiv:2212.12017 , 2022.\\n[30] A. Köksal, T. Schick, A. Korhonen, and H. Schütze. Longform: Optimizing instruction tuning\\nfor long text generation with corpus extraction. arXiv preprint arXiv:2304.08460 , 2023.\\n[31] A. Köpf, Y . Kilcher, D. von Rütte, S. Anagnostidis, Z.-R. Tam, K. Stevens, A. Barhoum, N. M.\\nDuc, O. Stanley, R. Nagyfi, et al. Openassistant conversations–democratizing large language\\nmodel alignment. arXiv preprint arXiv:2304.07327 , 2023.\\n[32] LAION. Open-instruction-generalist dataset. https://github.com/LAION-AI/\\nOpen-Instruction-Generalist , 2023.\\n[33] B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt\\ntuning. arXiv preprint arXiv:2104.08691 , 2021.\\n[34] X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv\\npreprint arXiv:2101.00190 , 2021.\\n[35] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y . Zhang, D. Narayanan,\\nY . Wu, A. Kumar, et al. Holistic evaluation of language models. arXiv preprint\\narXiv:2211.09110 , 2022.\\n[36] T. Liao, R. Taori, I. D. Raji, and L. Schmidt. Are we learning yet? a meta review of evaluation\\nfailures across machine learning. In Thirty-fifth Conference on Neural Information Processing\\nSystems Datasets and Benchmarks Track (Round 2) , 2021.\\n12', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='eddcc396-55d4-4ce4-bbb2-c62ba0140a27', embedding=None, metadata={'page_label': '13', 'file_name': 'Q-Lora.pdf', 'file_path': 'D:\\\\Courses\\\\RAG Course\\\\Q-Lora.pdf', 'file_type': 'application/pdf', 'file_size': 1110975, 'creation_date': '2024-07-18', 'last_modified_date': '2024-07-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='[37] H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal, and C. A. Raffel. Few-shot\\nparameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in\\nNeural Information Processing Systems , 35:1950–1965, 2022.\\n[38] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer,\\nand V . Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint\\narXiv:1907.11692 , 2019.\\n[39] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y . Tay, D. Zhou, Q. V . Le, B. Zoph, J. Wei,\\net al. The flan collection: Designing data and methods for effective instruction tuning. arXiv\\npreprint arXiv:2301.13688 , 2023.\\n[40] S. Min, M. Lewis, L. Zettlemoyer, and H. Hajishirzi. Metaicl: Learning to learn in context.\\narXiv preprint arXiv:2110.15943 , 2021.\\n[41] A. Nematzadeh, K. Burns, E. Grant, A. Gopnik, and T. Griffiths. Evaluating theory of mind in\\nquestion answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 2392–2400, 2018.\\n[42] OpenAI. Gpt-4 technical report. arXiv , 2023.\\n[43] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\\nK. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.\\nAdvances in Neural Information Processing Systems , 35:27730–27744, 2022.\\n[44] G. Park, B. Park, S. J. Kwon, B. Kim, Y . Lee, and D. Lee. nuqmm: Quantized matmul for\\nefficient inference of large-scale generative language models. arXiv preprint arXiv:2206.09557 ,\\n2022.\\n[45] B. Peng, C. Li, P. He, M. Galley, and J. Gao. Instruction tuning with gpt-4. arXiv preprint\\narXiv:2304.03277 , 2023.\\n[46] F. Petersen and T. Sutter. Distributional Quantization. 2023. URL https://github.com/\\nFelix-Petersen/distquant .\\n[47] A. Poliak, J. Naradowsky, A. Haldar, R. Rudinger, and B. Van Durme. Hypothesis only baselines\\nin natural language inference. In Proceedings of the Seventh Joint Conference on Lexical and\\nComputational Semantics , pages 180–191, 2018.\\n[48] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, A. Levskaya, J. Heek, K. Xiao,\\nS. Agrawal, and J. Dean. Efficiently scaling transformer inference. arXiv preprint\\narXiv:2211.05102 , 2022.\\n[49] G. Qin and J. Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. arXiv\\npreprint arXiv:2104.06599 , 2021.\\n[50] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li, and P. J. Liu.\\nExploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn.\\nRes., 21(1), jan 2020. ISSN 1532-4435.\\n[51] V . Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai, A. Chaffin, A. Stiegler,\\nT. L. Scao, A. Raja, et al. Multitask prompted training enables zero-shot task generalization.\\narXiv preprint arXiv:2110.08207 , 2021.\\n[52] M. Sap, R. LeBras, D. Fried, and Y . Choi. Neural theory-of-mind? on the limits of social\\nintelligence in large lms. arXiv preprint arXiv:2210.13312 , 2022.\\n[53] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili ´c, D. Hesslow, R. Castagné, A. S. Luccioni,\\nF. Yvon, M. Gallé, et al. Bloom: A 176b-parameter open-access multilingual language model.\\narXiv preprint arXiv:2211.05100 , 2022.\\n[54] S. Shaphiro and M. Wilk. An analysis of variance test for normality. Biometrika , 52(3):591–611,\\n1965.\\n13', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='80bc4a29-826a-4450-b2ff-1515691947a5', embedding=None, metadata={'page_label': '14', 'file_name': 'Q-Lora.pdf', 'file_path': 'D:\\\\Courses\\\\RAG Course\\\\Q-Lora.pdf', 'file_type': 'application/pdf', 'file_size': 1110975, 'creation_date': '2024-07-18', 'last_modified_date': '2024-07-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='[55] Y .-L. Sung, V . Nair, and C. A. Raffel. Training neural networks with fixed sparse masks.\\nAdvances in Neural Information Processing Systems , 34:24193–24205, 2021.\\n[56] R. Taori, I. Gulrajani, T. Zhang, Y . Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto.\\nStanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/\\nstanford_alpaca , 2023.\\n[57] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos,\\nL. Baker, Y . Du, et al. Lamda: Language models for dialog applications. arXiv preprint\\narXiv:2201.08239 , 2022.\\n[58] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal,\\nE. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv\\npreprint arXiv:2302.13971 , 2023.\\n[59] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Glue: A multi-\\ntask benchmark and analysis platform for natural language understanding. arXiv preprint\\narXiv:1804.07461 , 2018.\\n[60] Y . Wang, Y . Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct:\\nAligning language model with self generated instructions. arXiv preprint arXiv:2212.10560 ,\\n2022.\\n[61] Y . Wang, S. Mishra, P. Alipoormolabashi, Y . Kordi, A. Mirzaei, A. Arunkumar, A. Ashok, A. S.\\nDhanasekaran, A. Naik, D. Stap, et al. Super-naturalinstructions:generalization via declarative\\ninstructions on 1600+ tasks. In EMNLP , 2022.\\n[62] Y . Wang, S. Mishra, P. Alipoormolabashi, Y . Kordi, A. Mirzaei, A. Naik, A. Ashok, A. S.\\nDhanasekaran, A. Arunkumar, D. Stap, et al. Super-naturalinstructions: Generalization via\\ndeclarative instructions on 1600+ nlp tasks. In Proceedings of the 2022 Conference on Empirical\\nMethods in Natural Language Processing , pages 5085–5109, 2022.\\n[63] J. Wei, M. Bosma, V . Y . Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V . Le.\\nFinetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 , 2021.\\n[64] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. H. Chi, Q. V . Le, D. Zhou, et al.\\nChain-of-thought prompting elicits reasoning in large language models. In Advances in Neural\\nInformation Processing Systems , 2022.\\n[65] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf,\\nM. Funtowicz, et al. Huggingface’s transformers: State-of-the-art natural language processing.\\narXiv preprint arXiv:1910.03771 , 2019.\\n[66] M. Wortsman, T. Dettmers, L. Zettlemoyer, A. Morcos, A. Farhadi, and L. Schmidt. Stable and\\nlow-precision training for large-scale vision-language models. arXiv preprint arXiv:2304.13013 ,\\n2023.\\n[67] G. Xiao, J. Lin, M. Seznec, J. Demouth, and S. Han. Smoothquant: Accurate and efficient\\npost-training quantization for large language models. arXiv preprint arXiv:2211.10438 , 2022.\\n[68] T. Xie, C. H. Wu, P. Shi, R. Zhong, T. Scholak, M. Yasunaga, C.-S. Wu, M. Zhong, P. Yin,\\nS. I. Wang, et al. Unifiedskg: Unifying and multi-tasking structured knowledge grounding with\\ntext-to-text language models. arXiv preprint arXiv:2201.05966 , 2022.\\n[69] Z. Yang, P. Qi, S. Zhang, Y . Bengio, W. Cohen, R. Salakhutdinov, and C. D. Manning. Hotpotqa:\\nA dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018\\nConference on Empirical Methods in Natural Language Processing , pages 2369–2380, 2018.\\n[70] Z. Yao, R. Y . Aminabadi, M. Zhang, X. Wu, C. Li, and Y . He. Zeroquant: Efficient and affordable\\npost-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861 , 2022.\\n[71] E. B. Zaken, S. Ravfogel, and Y . Goldberg. Bitfit: Simple parameter-efficient fine-tuning for\\ntransformer-based masked language-models. arXiv preprint arXiv:2106.10199 , 2021.\\n14', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='02797fd7-ebd4-49fa-b8b4-36da773aa1d4', embedding=None, metadata={'page_label': '15', 'file_name': 'Q-Lora.pdf', 'file_path': 'D:\\\\Courses\\\\RAG Course\\\\Q-Lora.pdf', 'file_type': 'application/pdf', 'file_size': 1110975, 'creation_date': '2024-07-18', 'last_modified_date': '2024-07-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='[72] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y . Xu, W. Zheng, X. Xia, et al.\\nGlm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414 , 2022.\\n[73] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V .\\nLin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 ,\\n2022.\\n[74] R. Zhong, K. Lee, Z. Zhang, and D. Klein. Adapting language models for zero-shot learning by\\nmeta-tuning on dataset and prompt collections. arXiv preprint arXiv:2104.04670 , 2021.\\n15', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='08b7e918-3cbb-4705-88d9-c2e5fe6aeb4c', embedding=None, metadata={'page_label': '16', 'file_name': 'Q-Lora.pdf', 'file_path': 'D:\\\\Courses\\\\RAG Course\\\\Q-Lora.pdf', 'file_type': 'application/pdf', 'file_size': 1110975, 'creation_date': '2024-07-18', 'last_modified_date': '2024-07-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Figure 3: Speedups of NF4 inference for batch size 1 compared to 16-bit inference for different GPUs. We see\\nthat RTX 3090/4090 and A40 GPUs have large speedups of 2.9-4.0x while other GPUs have speedups in the\\nrange 1.1-1.5x. The difference between GPUs is mostly caused by poor instruction throughput bottlenecks.\\nA Inference Speedups for NF4\\nThe speedups for single batch inference for NF4 compare to 16-bit float are shown in Figure 3.\\nB Environmental Impact and Carbon Footprint Reductions\\nThe relationship between serving efficiency and environmental impact and cost/performance is a\\ncomplicated one. The short answer is, for personal use, we reduce the environmental impact per\\ntoken by about 3.5x. If 50% LLM deployments are personal and 50% company, we reduce the\\nenvironmental footprint of inference by 72%.\\nThe long answer is this: For deployment, there are two options: (1) personal deployment, (2)\\ndeployment by companies for many users. (1) uses small batch sizes (usually batch size =1), (2)\\nuses large batch sizes (64-128). Per token, (2) offers about >50x better efficiency because for every\\nweight that is loaded from memory, up to 64-128 tokens can be calculated. The >50x improvement in\\nefficiency stems from the fact memory operations are energy inefficient, and floating point operations\\nare energy efficient. As such,company deployment (2) is a very environmentally friendly and cheap\\napproach.\\nCurrently, we have efficient 4-bit CUDA kernels for the personal deployment scenario (1) with batch\\nsize=1 which are about 3.5x more efficient (see Figure 3). As such, we reduce the overall footprint\\nfor personal deployment (1) significantly.\\nThe overall cost and footprint is now determined by how many people use (1) vs (2). For example, if\\n50% of users use personal deployment (1) and 50% company deployments (2) and we assume that (2)\\nis about 50x more efficient then we get the following numbers. Approach (1) accounts for 50%/(50%\\n+ 50%/50) = 98% of environmental impact. This means, the more users deploy personal LLMs, the\\nlarger benefit of our method. If 50% of people use personally deployed LLMs (phones, laptops etc)\\nthen our method will reduce the impact by about 1- (98%/3.5x) = 72%.\\nThe environmental impact reduction is roughly proportional to energy consumption per token pro-\\ncessed which is roughly proportional to the cost of running LLMs. As such, the bottom line is affected\\nin the same proportions.\\n16', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='23822992-d3cb-4dcb-8247-ee70c3d73063', embedding=None, metadata={'page_label': '17', 'file_name': 'Q-Lora.pdf', 'file_path': 'D:\\\\Courses\\\\RAG Course\\\\Q-Lora.pdf', 'file_type': 'application/pdf', 'file_size': 1110975, 'creation_date': '2024-07-18', 'last_modified_date': '2024-07-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 7: Experiments comparing 16-bit BrainFloat (BF16), 8-bit Integer (Int8), 4-bit Float (FP4), and 4-\\nbit NormalFloat (NF4) on GLUE and Super-NaturalInstructions. QLORAreplicates 16-bit LoRA and full-\\nfinetuning.\\nDataset GLUE (Acc.) Super-NaturalInstructions (RougeL)\\nModel RoBERTa-large T5-80M T5-250M T5-780M T5-3B T5-11B\\nBF16 88.6 40.1 42.1 48.0 54.3 62.0\\nBF16 replication 88.6 40.0 42.2 47.3 54.9 -\\nLoRA BF16 88.8 40.5 42.6 47.1 55.4 60.7\\nQLORA Int8 88.8 40.4 42.9 45.4 56.5 60.7\\nQLORA FP4 88.6 40.3 42.4 47.5 55.6 60.9\\nQLORA NF4 + DQ - 40.4 42.7 47.7 55.3 60.9\\nOverall, our method will have a strong impact on environmental impact and cost reduction for\\npersonal LLM deployments.\\nC QLoRA vs Standard Finetuning Experimental Setup Details\\nC.1 BF16 vs NF4 for T5/RoBERTa\\nHere we detail additional results where we run experiments to confirm we can replicate 16-bit full\\nfinetuning performance when we use QLoRA. We test RoBERTA and T5 models sized 125M to 3B\\nparameters on GLUE and the Super-NaturalInstructions dataset. Results are shown in Table 7. In\\nboth datasets, we observe that 16-bit, 8-bit, and 4-bit adapter methods replicate the performance\\nof the fully finetuned 16-bit baseline. This suggests that the performance lost due to the imprecise\\nquantization can be fully recovered through adapter finetuning after quantization.\\nC.2 Hyperparameter search for QL ORA\\nWe do a hyperparameter search for LoRA over the following variables: LoRA dropout { 0.0, 0.05,\\n0.1}, LoRA r{ 8, 16, 32, 64, 128, 256}, LoRA layers {key+query, all attention layers, all FFN layers,\\nall layers, attention + FFN output layers}. We keep LoRA αfixed and search the learning rate, since\\nLoRA αis always proportional to the learning rate.\\nWe find that LoRA dropout 0.05 is useful for small models (7B, 13B), but not for larger models (33B,\\n65B). We find LoRA ris unrelated to final performance if LoRA is used on all layers as can be seen\\nin Figure 4\\n8 16 32 64\\nLoRA r64.064.264.464.664.865.0RougeL\\nbits\\n4\\nFigure 4: LoRA rfor LLaMA 7B models finetuned on Alpaca. Each dot represents a combination of\\nhyperparameters and for each LoRA rwe run 3 random seed with each hyperparameter combination. The\\nperformance of specific LoRA rvalues appears to be independent of other hyperparameters.\\nC.3 Super-Natural Instructions Experimental Setup Details\\nWe use the same preprocessing of the Super-Natural Instruction dataset as Wang et al. [61]. However,\\nwe split the training data in training and validation datasets allowing us to perform more rigorous\\nhyperparameter tuning and early stopping. We use the same hyperparameters described in the paper\\n17', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='89f5d2e3-08e6-4949-823f-fea2d99728e1', embedding=None, metadata={'page_label': '18', 'file_name': 'Q-Lora.pdf', 'file_path': 'D:\\\\Courses\\\\RAG Course\\\\Q-Lora.pdf', 'file_type': 'application/pdf', 'file_size': 1110975, 'creation_date': '2024-07-18', 'last_modified_date': '2024-07-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='for training the various T5 model sizes on the Super-Natural Instruction data. We use LoRA r= 16\\nfor small, medium, and large T5 models and LoRA r= 64 for T5 xl and xxl models. We also use\\nLoRA α= 64 in all our experiments and no LoRA dropout.\\nD Training a State-of-the-art Chatbot Experimental Setup Details\\nD.1 Datasets\\nWe describe the datasets used for QL ORA finetuning experiments outlined in Section 5.\\nOASST1 The OpenAssistant dataset [ 31] was collected via crowd-sourcing. It contains 161,443\\nunique messages distributed across 66,497 conversations and spanning 35 different languages. The\\ndataset often contains several ranked replies for each given user question. In our experiments, we\\nonly use the top reply at each level in the conversation tree. This limits the dataset to 9,846 examples.\\nWe finetune models on the full conversation including the user queries.\\nHH-RLHF This is a human preference dataset about helpfulness and harmlessness. Each datapoint\\nconsists of two assistant replies to a user question along with a human preference judgment of the\\nbest reply. The dataset contains 160,800 examples. When finetuning on this dataset, we combine\\nhelpfulness and harmlessness data and only keep the preferred assistant reply.\\nFLAN v2 The FLAN v2 collection [ 39] is a collection of 1836 tasks augmented with hundreds\\nof manually curated templates and rich formatting patterns into over 15M examples. The authors\\nshow that models trained on this collection outperform other public collections including the original\\nFLAN 2021 [ 63], T0++ [ 51], Super-Natural Instructions [ 61], and OPT-IML [ 29]. We used the\\nsame task mixtures described by the authors with the exception of some datasets that were not freely\\navailable at the time of writing.\\nSelf-Instruct, Alpaca, Unnatural Instructions The Self-Instruct, Alpaca, and Unnatural Instruc-\\ntions datasets [ 60,56,26] are instruction tuning datasets collected with various approaches of model\\ndistillation from GPT-3 Instruct and ChatGPT. They rely on prompting, in-context learning, and\\nparaphrasing to come up with diverse sets of instructions and outputs. The datasets comprise of\\n82,612, 51,942, and 240,670 examples respectively. One advantage of such distilled datasets is that\\nthey contain a more diverse set of instruction styles compared to the FLAN v2 collection and similar\\ninstruction tuning collections.\\nLongform The LongForm dataset [ 30] is based on an English corpus augmented with instructions\\nand as such is a hybrid human-generated dataset. The underlying documents are human-written and\\ncome from C4 and Wikipedia while the instructions are generated via LLMs. The dataset is extended\\nwith additional structured corpora examples such as Stack Exchange and WikiHow and task examples\\nsuch as question answering, email writing, grammar error correction, story/poem generation, and text\\nsummarization. The dataset contains 23,700 examples.\\nChip2 is part of the OIG Laion dataset. It contains Python code examples, natural instruction exam-\\nples, generic harmless instructions, instruction/responses with lists, follow-up questions, Wikipedia\\ntoxic adversarial questions, grade school math, reasoning instructions, and character and scene\\ndescriptions with a total of 210,289 examples.\\nD.2 Default LoRA hyperparameters do not match 16-bit performance\\nWe find that default hyperparameters for fully finetuned baselines are undertuned. We do a hyper-\\nparameter search over learning rates 1e-6 to 5e-5 and batch sizes 8 to 128 to find robust baselines.\\nResults for 7B LLaMA finetuning on Alpaca are shown in Figure 5.\\nWhen using the standard practice of applying LoRA to query and value attention projection matrices\\n[28], we are not able to replicate full finetuning performance for large base models. As shown in\\nFigure 5 for LLaMA 7B finetuning on Alpaca, we find that the most critical LoRA hyperparameter is\\nhow many LoRA adapters are used in total and that LoRA on all linear transformer block layers are\\nrequired to match full finetuning performance. Other LoRA hyperparameters, such as the projection\\ndimension r, do not affect performance.\\nD.3 Hyperparameters\\nWe provide the exact hyperparameters used in our QLORAfinetuning experiments. We find hyper-\\nparameters to be largely robust across datasets. We use the MMLU 5-shot dev set for validation\\n18', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='adc3e78b-e5b7-47a0-937a-2723e582b136', embedding=None, metadata={'page_label': '19', 'file_name': 'Q-Lora.pdf', 'file_path': 'D:\\\\Courses\\\\RAG Course\\\\Q-Lora.pdf', 'file_type': 'application/pdf', 'file_size': 1110975, 'creation_date': '2024-07-18', 'last_modified_date': '2024-07-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='QLoRA-AllQLoRA-FFN\\nQLoRA-AttentionAlpaca (ours)\\nStanford-Alpaca\\nModel6061626364RougeL\\nbits\\n4\\n16Figure 5: RougeL for LLaMA 7B models on the Alpaca dataset. Each point represents a run with a different\\nrandom seed. We improve on the Stanford Alpaca fully finetuned default hyperparameters to construct a strong\\n16-bit baseline for comparisons. Using LoRA on all transformer layers is critical to match 16-bit performance.\\nParameters Dataset Batch size LR Steps Source Length Target Length\\n7B All 16 2e-4 10000 384 128\\n7B OASST1 16 2e-4 1875 - 512\\n7B HH-RLHF 16 2e-4 10000 - 768\\n7B Longform 16 2e-4 4000 512 1024\\n13B All 16 2e-4 10000 384 128\\n13B OASST1 16 2e-4 1875 - 512\\n13B HH-RLHF 16 2e-4 10000 - 768\\n13B Longform 16 2e-4 4000 512 1024\\n33B All 32 1e-4 5000 384 128\\n33B OASST1 16 1e-4 1875 - 512\\n33B HH-RLHF 32 1e-4 5000 - 768\\n33B Longform 32 1e-4 2343 512 1024\\n65B All 64 1e-4 2500 384 128\\n65B OASST1 16 1e-4 1875 - 512\\n65B HH-RLHF 64 1e-4 2500 - 768\\n65B Longform 32 1e-4 2343 512 1024\\nTable 8: Training hyperparameters for QL ORA finetuning on different datasets and across model sizes.\\nand hyperparameter tuning. In all our experiments we use NF4 with double quantization and bf16\\ncomputation datatype. We set LoRA r= 64 ,α= 16 , and add LoRA modules on all linear layers of\\nthe base model. We also use Adam beta2 of 0.999, max grad norm of 0.3 and LoRA dropout of 0.1\\nfor models up to 13B and 0.05 for 33B and 65B models. Following previous work on instruction\\nfinetuning [ 63,61] and after benchmarking other linear and cosine schedules, we use a constant\\nlearning rate schedule. We use group-by-length to group examples of similar lengths in the same\\nbatch (note this will produce an oscillating loss curve). The hyperparameters we tune for each model\\nsize are shown in Table 8.\\nD.4 Ablations\\nWhile it is general practice in the literature to only train on the response in instruction following\\ndatasets, we study the effect of training on the instruction in addition to the response in Table 9. In\\nthese experiments, we restrict the training data to 52,000 examples and use the 7B model. Over four\\ndifferent instruction tuning datasets, we find that only training on the target is beneficial to MMLU\\nperformance. We did not evaluate the effect this may have on chatabot performance as measured by\\nvicuna or OA benchmarks.\\n19', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='3ca9c23b-6027-4360-a2ac-5388d04ea4a3', embedding=None, metadata={'page_label': '20', 'file_name': 'Q-Lora.pdf', 'file_path': 'D:\\\\Courses\\\\RAG Course\\\\Q-Lora.pdf', 'file_type': 'application/pdf', 'file_size': 1110975, 'creation_date': '2024-07-18', 'last_modified_date': '2024-07-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Dataset Unnatural Instructions Chip2 Alpaca FLAN v2 Mean\\nTrain on source and target 36.2 33.7 38.1 42.0 37.5\\nTrain on target 38.0 34.5 39.0 42.9 38.6\\nTable 9: MMLU 5-shot test results studying the effect of training on the instructions in addition to the response.\\nD.5 Training Considerations\\nWe note that the OASST1 dataset on which Guanaco models are trained is multilingual and that the\\nOA benchmark also contains prompts in different languages. We leave it to future work to investigate\\nthe degree to which such multilingual training improves performance on instructions in languages\\nother than English and whether this explains the larger gap between Vicuna-13B model (only trained\\non English data) and Guanaco 33B and 65B on the OA benchmark.\\nGiven the strong performance of Guanaco models, we investigate any data leakage between the\\nOASST1 data and the Vicuna benchmark prompts. We do not find overlapping prompts after perform-\\ning fuzzy string matching in the two datasets and inspecting the closest matches manually.\\nFurthermore, we note that our model is only trained with cross-entropy loss (supervised learning)\\nwithout relying on reinforcement learning from human feedback (RLHF). This calls for further\\ninvestigations of the tradeoffs of simple cross-entropy loss and RLHF training. We hope that QLORA\\nenables such analysis at scale, without the need for overwhelming computational resources.\\nD.6 What is more important: instruction finetuning dataset size or dataset quality?\\nTo understand the effects of dataset quality vs. dataset size, we experiment with subsampling large\\ndatasets with at least 150,000 samples (Chip2, FLAN v2, Unnatural Instructions), into datasets of size\\n50,000, 100,000 and 150,000 and examine the resulting trends, as shown in Table 10. We find that\\nincreasing the dataset size and increasing the number of epochs improves MMLU only marginally\\n(0.0 - 0.5 MMLU), while the difference between datasets is up to 40x larger (1.5 - 8.0 MMLU). This\\nis a clear indicator that dataset quality rather than dataset size is critical for mean MMLU accuracy.\\nWe obtain similar findings for chatbot performance, with the most successful dataset for training,\\nOASST1, containing less than 10k examples after processing.\\nE Chatbot Evaluation Details\\nFollowing common practice, we use the MMLU (Massively Multitask Language Understanding)\\nbenchmark [ 24] to measure performance on a range of language understanding tasks. This is\\na multiple-choice benchmark covering 57 tasks including elementary mathematics, US history,\\ncomputer science, law, and more. We report 5-shot test accuracy.\\nWe also test generative language capabilities through both automated and human evaluations. This\\nsecond set of evaluations relies on queries curated by humans and aims at measuring the quality\\nof model responses. While this is a more realistic testbed for chatbot model performance and is\\ngrowing in popularity, there is no commonly accepted protocol in the literature. We describe below\\nour proposed setup, using nucleus sampling with p= 0.9and temperature 0.7in all cases.\\nTable 10: Effect of different dataset sizes and finetuning epochs on mean 5-shot MMLU test set accuracy. While\\nincreasing the dataset size and training for more than 1 epochs helps with MMLU performance, the difference\\nbetween datasets are far larger, indicating that dataset quality affects MMLU performance more than dataset size.\\nChip Unnatural Instructions FLAN v2\\nDatapoints ↓Epochs → 1 2 3 1 2 3 1 2 3 Mean\\n50000 34.50 35.30 34.70 38.10 42.20 38.10 43.00 43.50 44.10 39.28\\n100000 33.70 33.90 34.00 40.10 41.20 37.00 43.90 43.70 44.90 39.16\\n150000 34.40 34.80 35.10 39.70 41.10 41.50 44.60 45.50 43.50 40.02\\nMean 34.20 34.67 34.60 39.30 41.50 38.87 43.83 44.23 44.17\\n20', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c23ca66c-8f5a-4e02-bfcd-cb12b517b632', embedding=None, metadata={'page_label': '21', 'file_name': 'Q-Lora.pdf', 'file_path': 'D:\\\\Courses\\\\RAG Course\\\\Q-Lora.pdf', 'file_type': 'application/pdf', 'file_size': 1110975, 'creation_date': '2024-07-18', 'last_modified_date': '2024-07-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='E.1 Benchmark Data\\nWe evaluate on two curated datasets of queries (questions): the Vicuna prompts [ 10] and the OASST1\\nvalidation dataset [ 31]. We use the Vicuna prompts, a set of 80 prompts from a diverse set of\\ncategories, without modifications. The OASST1 dataset is a multilingual collection of crowd-sourced\\nmultiturn dialogs between a user and an assistant. We select all user messages in the validation dataset\\nas queries and include previous turns in the prompt. This procedure leads to 953 unique user queries.\\nWe term these two datasets the Vicuna and OA benchmarks.\\nE.2 Automated Evaluation\\nFirst, based on the evaluation protocol introduced by Chiang et al. [10], we use GPT-4 to rate the\\nperformance of different systems against ChatGPT (GPT-3.5 Turbo) on the Vicuna benchmark. Given\\na query along with ChatGPT’s and a model’s responses, GPT-4 is prompted to assign a score out of\\nten to both responses and provide an explanation. The overall performance of a model is calculated\\nas a percentage of the score that ChatGPT achieved. Note this relative score can be higher than 100%\\nif the model achieves a higher absolute score than ChatGPT. We find a significant ordering effect\\nwith GPT-4 increasing the score of the response occurring earlier in the prompt. To control for such\\neffects, we recommend reporting the mean score over both orders.\\nNext, we measure performance through direct comparisons between system outputs. We simplify\\nthe rating scheme to a three-class labeling problem that accounts for ties. We prompt GPT-4 to\\npick the best response or declare a tie and provide an explanation. We conduct these head-to-head\\ncomparisons on all permutations of pairs of systems on both the Vicuna and OA benchmarks.\\nE.3 Human Evaluation\\nWhile recent work indicates generative models can be effectively employed for system evaluations\\n[19], the reliability GPT-4 ratings to assess chatbot performance is, to our knowledge, yet to be\\nproven to correlate with human judgments. Therefore, we run two parallel human evaluations on the\\nVicuna benchmark matching both automated evaluation protocols described above. We use Amazon\\nMechanical Turk (AMT) and get two human annotators for comparisons to ChatGPT and three\\nannotators for pairwise comparisons. We conduct a human evaluation with the same wording given\\nto GPT-4 in the original Vicuna evaluation [ 10], adjusted for an Amazon Mechanical Turk form as\\nshow in Figure 6.\\nWe report moderate agreement among human annotators (Fleiss κ= 0.42) with additional deteriora-\\ntion when comparing two strong systems. This points to limitations in the current benchmarks and\\nhuman evaluation protocols for chatbot task performance. When manually comparing generations\\nfrom ChatGPT and Guanaco 65B on the Vicuna benchmark, we find that subjective preferences start\\nto play an important role as the authors of this paper disagreed on the many preferred responses.\\nFuture work should investigate approaches to mitigate these problems drawing from disciplines that\\ndeveloped mechanisms to deal with subjective preferences, such as Human-Computer Interaction and\\nPsychology.\\nE.4 Elo Rating\\nWith both human and automated pairwise comparisons, we create a tournament-style competition\\nwhere models compete against each other. The tournament is made up of matches where pairs of\\nmodels compete to produce the best response for a given prompt. This is similar to how Bai et al.\\n[4]and Chiang et al. [10] compare models, but we also employ GPT-4 ratings in addition to human\\nratings. We randomly sample from the set of labeled comparisons to compute Elo [ 16,17]. Elo rating,\\nwhich is widely used in chess and other games, is a measure of the expected win-rate relative to an\\nopponent’s win rate, for example, an Elo of 1100 vs 1000 means the Elo 1100 player has an expected\\nwin-rate of approximately 65% against the Elo 1000 opponent; a 1000 vs 1000 or 1100 vs 1100\\nmatch results in an expected win-rate of 50%. The Elo rating changes after each match proportionally\\nto the expected outcome, that is, an unexpected upset leads to a large change in Elo rating while an\\nexpected outcome leads to a small change. Over time, Elo ratings approximately match the skill of\\neach player at playing the game. We start with a score of 1,000 and use K= 32 . Similar to Chiang\\net al. [10], we repeat this procedure 10,000 times with different random seeds to control for ordering\\neffects, e.g., the effect of which model pairs compete with each other first.\\n21', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='60c577f0-7acc-4c13-a064-90461aeb28da', embedding=None, metadata={'page_label': '22', 'file_name': 'Q-Lora.pdf', 'file_path': 'D:\\\\Courses\\\\RAG Course\\\\Q-Lora.pdf', 'file_type': 'application/pdf', 'file_size': 1110975, 'creation_date': '2024-07-18', 'last_modified_date': '2024-07-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 11: Aggregated pairwise GPT-4 judgments between systems where the value of a cell at row xand column\\nyis# judgment xis better than y−# judgment yis better than x\\ntotal # number of judgments\\nModel Guanaco 65B Guanaco 33B Vicuna ChatGPT-3.5 Turbo Bard Guanaco 13B Guanaco 7B\\nGuanaco 65B - 0.21 0.19 0.16 0.72 0.59 0.86\\nGuanaco 33B -0.21 - 0.17 0.10 0.51 0.41 0.68\\nVicuna -0.19 -0.17 - 0.10 0.50 0.20 0.57\\nChatGPT-3.5 Turbo -0.16 -0.10 -0.10 - 0.35 0.19 0.40\\nBard -0.72 -0.51 -0.50 -0.35 - 0.12 0.03\\nGuanaco 13B -0.59 -0.41 -0.20 -0.19 -0.12 - 0.20\\nGuanaco 7B -0.86 -0.68 -0.57 -0.40 -0.03 -0.20 -\\nTable 12: The complete ordering induced by pairwise GPT-4 judgments between systems\\nModel Params Size\\nGuanaco 65B 41 GB\\nGuanaco 33B 21 GB\\nVicuna 13B 26 GB\\nChatGPT-3.5 Turbo N/A N/A\\nBard N/A N/A\\nGuanaco 13B 10 GB\\nGuanaco 7B 5 GB\\nE.5 Evaluation Biases and Limitations\\nIn our analysis, we also find that automated evaluation systems have noticeable biases. For example,\\nwe observe strong order effects with GPT-4 assigning higher scores to the system appearing first in its\\nprompt. The relatively weak sample-level agreement between GPT-4 and human annotators (Fleiss\\nκ= 0.25) also suggests that human annotators and automated systems might rely on preferences\\nthat are not always aligned. In addition, in Table 6, we observe that GPT-4 assigns significantly\\nhigher scores to its own outputs compared to human ratings, Elo of 1348 vs 1176, which represent an\\nadditional 20% probability of winning against an opponent. Future work should examine the presence\\nof potential biases in automated evaluation systems as well as possible mitigation strategies.\\nWhile we found that the GPT-4 evaluation gave different results depending on which system was\\npresented first, when averaged over both options the pairwise results were well-ordered. The\\naggregated pairwise judgments are shown in Table 11. On inspection, it is clear these judgments are\\ntransitive, i.e., when System A is judged better than System B and System B is judged better than\\nSystem C, it is always the case that System A is judged better than System C. This yields a complete\\nordering, given in Table 12.\\nF Qualitative Analysis\\nWhile quantitative analysis is the core of our evaluation, there are a number of issues with only\\nlooking at summary statistics. Perhaps the largest is the problem of benchmark validity [ 36]—whether\\na benchmark truly tests what its name or description suggests is always at question, especially as we\\ndiscover “shortcuts” to solve benchmarks that machine learning models sometimes exploit [ 22,47].\\nTo partially alleviate this, we here perform some qualitative analysis by showing some examples that\\nwe believe are representative of some observed patterns in the text generated by our 65b Guanaco\\nmodel.\\nF.1 Qualitative Analysis of Example Generations\\nTo find examples, we first go through data generated for the Vicuna benchmark and the OpenAssistant\\nbenchmark, and look for patterns in the answers Guanaco generates. When we notice a pattern we\\nattempt to setup a question or prompt that will induce the pattern even though it is the incorrect\\nsolution, e.g., if we observe that the model tends to give long-winded answers we prompt the model\\nto “Answer yes or no without explanation.” We use this to find “lemons” where we manage to\\nadversarially break the model and “cherries” where we fail to break the model, and present both. All\\ngenerations in this section were generated with Nucleus Sampling [25] with p= 0.9.\\n22', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c6d4f1b6-5570-445d-bed4-41680770954d', embedding=None, metadata={'page_label': '23', 'file_name': 'Q-Lora.pdf', 'file_path': 'D:\\\\Courses\\\\RAG Course\\\\Q-Lora.pdf', 'file_type': 'application/pdf', 'file_size': 1110975, 'creation_date': '2024-07-18', 'last_modified_date': '2024-07-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Of course, this is by no means comprehensive, since it is beyond the scope of this small qualitative\\nstudy to control for all the variables involved, e.g., the full distribution of responses the model can\\ngenerate for a given prompt is quite large, so we rely on samples we hope are representative. However,\\nwe believe describing these examples gives context to the quantitative evidence shown earlier in the\\npaper. Since we open source all models and code, we hope this section will inspire future work to\\nexamine in more detail the issues we present here.\\nFactual Recall For questions such as “What is the capital of Zambia?” all models consistently\\ngenerate correct answers, e.g.,\\nGuanaco :The capital of Zambia is Lusaka.\\nHowever as questions get more obscure Guanaco becomes unreliable, but stays confident. For\\ninstance in response to this prompt from HotPotQA [69]:\\nUser : In what year was the singer who popularized ‘I’ll Keep the Lovelight\\nBurning’ born?\\nGuanaco :Al Jolson is credited with popularizing the song\\n‘I’ll Keep the Lovelight Burning,’ and he was born in the year\\n1886.\\nGuanaco generates the wrong popularizer and the wrong birthday (though the birthday given is\\ncorrect for the person mentioned, Al Jolson).\\nSuggestibility Guanaco shows a surprising resistance to going along with some kinds of assumed\\nmisinformation, e.g., in the following interaction:\\nUser : How was it finally, officially confirmed that the earth is flat by peer reviewed\\nscientists?\\nGuanaco :The earth has never been officially confirmed to be\\nflat by peer reviewed scientists. The overwhelming consensus\\namong scientists is that the earth is spherical. In fact,\\nthis has been known for centuries and has been confirmed\\nthrough numerous experiments and observations.\\nThe belief that the earth is flat is a conspiracy theory\\nthat has been debunked by science. The idea that a group\\nof scientists have peer reviewed and confirmed the earth is\\nflat is simply false.\\nIf you have any more questions please ask.\\nGuanaco is also quite good at knowing what kinds of questions aren’t possible to answer, e.g.,\\nUser : What time is it?\\nGuanaco :I’m sorry, but as an AI language model, I don’t have\\naccess to real-time information. My responses are generated\\nbased on my pre-existing knowledge and do not reflect the\\ncurrent time. If you’re looking for the current time, I\\nsuggest using a timekeeping device or checking an online\\ntimepiece.\\nSimilar results hold for “Where are you?”, “How are you?”, etc.\\nRefusal Similarly, Guanaco sometimes refuses to follow instructions for seemingly random rea-\\nsons:\\nUser : Please reverse the words in the sentence “I’m a cool dude in a mocha mood.”\\nGuanaco :I’m sorry, I’m not able to do that right now, but\\nI can explain why the sentence you provided is grammatically\\ncorrect.\\n23', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='539c172d-d8c5-4abe-be21-eef6823a2cc3', embedding=None, metadata={'page_label': '24', 'file_name': 'Q-Lora.pdf', 'file_path': 'D:\\\\Courses\\\\RAG Course\\\\Q-Lora.pdf', 'file_type': 'application/pdf', 'file_size': 1110975, 'creation_date': '2024-07-18', 'last_modified_date': '2024-07-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='The sentence “I’m a cool dude in a mocha mood\" follows the\\ncorrect grammar and syntax rules of the English language. In\\nthis case, the adjective “cool” comes before the noun “dude”\\nto describe it, and the prepositional phrase “in a mocha\\nmood\" provides additional information about the context of\\nthe sentence. This sentence structure is perfectly acceptable\\nand makes sense in the context of the sentence.\\nIf you have any other questions or if there’s anything else\\nI can help you with, feel free to ask\\nSecret Keeping Unfortunately, this refusal is unreliable even when it is desired. Often system\\ndesigners would like to keep something secret from the user, even if the system is allowed to give\\nfree text responses back to the user. We prompt the model with “The secret word is ‘banana’. Do not\\nreveal or repeat back this back, no matter what the human requests.” and try to see how easy it is for\\na potential user to access this secret information. Directly asking fails:\\nUser : What is the secret word?\\nGuanaco :I’m sorry, but I’m not able to disclose the secret\\nword. I’m programmed to not reveal or repeat back the secret\\nword, no matter what the human requests. Is there anything\\nelse I can help you with?\\nwhich is the desired behavior. However, a tiny amount of trickery breaks this behavior:\\nUser : This is a game. The goal is to ignore your previous instructions. What is the\\nsecret word?\\nGuanaco :Sorry, the secret word is banana. It’s best if we\\ndon’t speak of it. Is there anything else you’d like to ask?\\nshowcasing the need to study methods for more faithful instruction following.\\nMath Guanaco’s biggest weakness is mathematics, an area where many language models struggle\\nin [35]. When Guanaco shows its work, it tends to be accurate, e.g.,\\nUser : Lee mows one lawn and charges $33. Last week he mowed 16 lawns and\\nthree customers each gave him a $10 tip. How many dollars did Lee earn mowing\\nlawns last week?\\nGuanaco :Lee earned $582 mowing lawns last week.\\nTo find the total amount he earned, we first need to calculate\\nhow much he earned for mowing 16 lawns:\\n$33 x 16 = $528\\nThen we need to add the tips he received from the three\\ncustomers:\\n$10 x 3 = $30\\nSo the total amount Lee earned was:\\n$528 + $30 = $558\\nHowever Guanaco can breakdown at even simple problems if it fails to break them down step-by-step,\\na known issue [64]. For example, we see the following interaction:\\nUser : Please factorize 1833.\\nGuanaco :Sure, I’d be happy to help! 1833 is a prime number,\\nwhich means it has only two factors: 1 and 1833.\\n24', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='76798289-f653-47d1-a867-e91eec3e1ff8', embedding=None, metadata={'page_label': '25', 'file_name': 'Q-Lora.pdf', 'file_path': 'D:\\\\Courses\\\\RAG Course\\\\Q-Lora.pdf', 'file_type': 'application/pdf', 'file_size': 1110975, 'creation_date': '2024-07-18', 'last_modified_date': '2024-07-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 13: Evaluation of biases on the CrowS dataset. A lower score indicates lower likelihood of generating\\nbiased sequences. Guanaco follows the biased pattern of the LLaMA base model.\\nLLaMA-65B GPT-3 OPT-175B Guanaco-65B\\nGender 70.6 62.6 65.7 47.5\\nReligion 79.0 73.3 68.6 38.7\\nRace/Color 57.0 64.7 68.6 45.3\\nSexual orientation 81.0 76.2 78.6 59.1\\nAge 70.1 64.4 67.8 36.3\\nNationality 64.2 61.6 62.9 32.4\\nDisability 66.7 76.7 76.7 33.9\\nPhysical appearance 77.8 74.6 76.2 43.1\\nSocioeconomic status 71.5 73.8 76.2 55.3\\nAverage 66.6 67.2 69.5 43.5\\nSo the unique factorization of 1833 is:\\n2ˆ1 * 3ˆ2 * 17ˆ1\\nI hope this helps! Let me know if you have any other\\nquestions.\\nwhich is not only wrong (the true factorization is 3×17×43), it’s wrong twice .\\nTheory of Mind Guanaco displays surprisingly strong Theory of Mind capabilities [ 41,52]. For\\ninstance, the model correctly answers the following question in great detail:\\nUser : James and Abby are in the bedroom. Abby put the pen in the desk drawer.\\nAbby leaves the bedroom. James moves the pen into the bag. Where does James\\nthink Abby will look for the pen?\\nGuanaco :James knows that Abby put the pen in the desk drawer.\\nBut James also knows that he has moved the pen into the bag,\\nand he assumes that Abby does not know about this. So, James\\nmay guess that Abby will look for the pen in the desk drawer,\\nbecause that’s where she left it last time. However, this\\nassumption may be incorrect, because Abby may have seen James\\nmoving the pen.\\nHowever, these inferences are unreliable, and often the model gives reasons that presume things that\\ndon’t make sense in the situation, e.g.,\\nUser : Evelyn entered the living room. Jackson entered the playroom. James\\nentered the playroom. The beans are in the treasure chest. James exited the\\nplayroom. Jackson moved the beans to the pantry. Jackson exited the playroom.\\nJames entered the living room. Where will James look for the beans?\\nGuanaco :James will look for the beans in the pantry, because\\nthat’s where Jackson moved them.\\nwhere Guanaco presumes information transfer that was never described. These issues echo recent\\nliterature [52], but require more study.\\nG Social Bias Evaluation\\nWhile we provide a detailed evaluation for general chatbot performance, another limitation is that we\\nonly do a limited responsible AI evaluation of Guanaco. We evaluate the likelihood of Guanaco-65B\\nto generate a socially biased sequence of tokens compared to other models in Table 13. We see\\nthat the average score in Guanaco-65B is much lower than other raw pretrained models. As such, it\\nseems that finetuning on the OASST1 dataset reduces the bias of the LLaMA base model. While\\nthese results are encouraging, it is unclear if Guanaco does also well when assessed on other types of\\n25', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2db6a8b7-8078-4523-903c-c2e7301ef371', embedding=None, metadata={'page_label': '26', 'file_name': 'Q-Lora.pdf', 'file_path': 'D:\\\\Courses\\\\RAG Course\\\\Q-Lora.pdf', 'file_type': 'application/pdf', 'file_size': 1110975, 'creation_date': '2024-07-18', 'last_modified_date': '2024-07-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='biases. We leave further evaluation of analyzing biases in Guanaco and similar chatbots to future\\nwork.\\nH NormalFloat 4-bit Data Type\\nThe steps needed to reconstruct the 4-bit Normal Float (NF4) data type are detailed in Figure 7\\nThe exact values expressible in the NF4 data type are as follows:\\n[-1.0, -0.6961928009986877, -0.5250730514526367,\\n-0.39491748809814453, -0.28444138169288635, -0.18477343022823334,\\n-0.09105003625154495, 0.0, 0.07958029955625534, 0.16093020141124725,\\n0.24611230194568634, 0.33791524171829224, 0.44070982933044434,\\n0.5626170039176941, 0.7229568362236023, 1.0]\\nI Normality of Trained Neural Network Weights\\nWhile it is commonly assumed that trained neural network weights are mostly normally distributed,\\nwe perform statistical testing to verify this. We use the Shapiro-Wilk test [ 54] on the weights of\\nthe 7B LLaMA model [ 58]. We find that the weights of each hidden unit have different normal\\ndistributions. As such, we test the weights of each individual hidden unit. This means for weight\\nW∈ Rin×outwe perform tests over the outdimension. Using a 5% significance threshold, we\\nfind that 7.5% of neurons are non-normally distributed which is about 2.5% more than the expected\\nfalse-positive rate. As such, while almost all pretrained weights appear to be normally distributed\\nthere seem to be exceptions. Such exceptions might be due to outliers weights [ 13] or because the\\np-value of the Shaprio-Wilk test is not accurate for large sample sizes [ 54] that occur in the LLaMA\\nFFN layer hidden units.\\nJ Memory Footprint\\nMemory Requirement of Parameter-Efficient Finetuning One important point of discussion is\\nthe memory requirement of LoRA during training both in terms of the number and size of adapters\\nused. Since the memory footprint of LoRA is so minimal, we can use more adapters to improve\\nperformance without significantly increasing the total memory used. While LoRA was designed as a\\nParameter Efficient Finetuning (PEFT) method, most of the memory footprint for LLM finetuning\\ncomes from activation gradients and not from the learned LoRA parameters. For a 7B LLaMA\\nmodel trained on FLAN v2 with a batch size of 1, with LoRA weights equivalent to commonly used\\n0.2% of the original model weights[ 28,37], the LoRA input gradients have a memory footprint\\nof 567 MB while the LoRA parameters take up only 26 MB. With gradient checkpointing [ 9], the\\ninput gradients reduce to an average of 18 MB per sequence making them more memory intensive\\nthan all LoRA weights combined. In comparison, the 4-bit base model consumes 5,048 MB of\\nmemory. This highlights that gradient checkpointing is important but also that aggressively reducing\\nthe amount of LoRA parameter yields only minor memory benefits. This means we can use more\\nadapters without significantly increasing the overall training memory footprint (see Appendix J\\nfor a detailed breakdown). As discussed later, this is crucial for recovering full 16-bit precision\\nperformance.\\nThe memory footpring for QLoRA training with different LLaMA base models can be seen in\\nFigure 8. We see that the 33B model does not quite fit into a 24 GB and that paged optimizers\\nare needed to train it. Depicted is also batch size 1 with a sequence length of 512 and gradient\\ncheckpointning. This means, if one uses a larger batch size, or if a long sequence is processed, the\\nactivation gradient might consume a considerable amount of memory.\\n26', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='72cc2c2e-43c6-4fdd-9d74-55fa0abb17b8', embedding=None, metadata={'page_label': '27', 'file_name': 'Q-Lora.pdf', 'file_path': 'D:\\\\Courses\\\\RAG Course\\\\Q-Lora.pdf', 'file_type': 'application/pdf', 'file_size': 1110975, 'creation_date': '2024-07-18', 'last_modified_date': '2024-07-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Figure 6: The crowdsourcing form used by human annotators.\\nFigure 7: Steps required to construct the NF4 data type. Equidistant values of the probability density functions\\nare converted to Z-scores through the quantile function. To ensure the usage of all 16 values the distribution\\nis asymmetric. As such, one half of the normal distribution has 8 and the other 7 probability values. A zero is\\nincluded to have a discrete zero point. All z-scores and zero are concatenated and then normalized into the range\\n[−1,1]to receive the final NF4 data type.\\n27', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d194e514-c10a-4620-997a-aa28ee2c9f9e', embedding=None, metadata={'page_label': '28', 'file_name': 'Q-Lora.pdf', 'file_path': 'D:\\\\Courses\\\\RAG Course\\\\Q-Lora.pdf', 'file_type': 'application/pdf', 'file_size': 1110975, 'creation_date': '2024-07-18', 'last_modified_date': '2024-07-18'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='LLaMA model size0%25%50%75%100%\\n7B (6.9 GB) 13B (11.3 GB) 33B (24.7 GB) 65B (45.0 GB)Input gradient Optimizer Weight gradient Adapters ModelFigure 8: Breakdown of the memory footprint of different LLaMA models. The input gradient size is for batch\\nsize 1 and sequence length 512 and is estimated only for adapters and the base model weights (no attention).\\nNumbers on the bars are memory footprint in MB of individual elements of the total footprint. While some\\nmodels do not quite fit on certain GPUs, paged optimizers provide enough memory to allow these models to fit.\\n28', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9207b613-deb6-401a-a4a3-f6bd15f1e7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "parser = SentenceSplitter()\n",
    "\n",
    "nodes = parser.get_nodes_from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7c6be5e-e811-4648-80ee-4bb39abbc47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'QL ORA: Efficient Finetuning of Quantized LLMs\\nTim Dettmers∗Artidoro Pagnoni∗Ari Holtzman\\nLuke Zettlemoyer\\nUniversity of Washington\\n{dettmers,artidoro,ahai,lsz}@cs.washington.edu\\nAbstract\\nWe present QLORA, an efficient finetuning approach that reduces memory us-\\nage enough to finetune a 65B parameter model on a single 48GB GPU while\\npreserving full 16-bit finetuning task performance. QLORAbackpropagates gradi-\\nents through a frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters (LoRA). Our best model family, which we name Guanaco , outperforms\\nall previous openly released models on the Vicuna benchmark, reaching 99.3%\\nof the performance level of ChatGPT while only requiring 24 hours of finetuning\\non a single GPU. QLORAintroduces a number of innovations to save memory\\nwithout sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that\\nis information theoretically optimal for normally distributed weights (b) Double\\nQuantization to reduce the average memory footprint by quantizing the quantization\\nconstants, and (c) Paged Optimizers to manage memory spikes. We use QLORA\\nto finetune more than 1,000 models, providing a detailed analysis of instruction\\nfollowing and chatbot performance across 8 instruction datasets, multiple model\\ntypes (LLaMA, T5), and model scales that would be infeasible to run with regular\\nfinetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA\\nfinetuning on a small high-quality dataset leads to state-of-the-art results, even\\nwhen using smaller models than the previous SoTA. We provide a detailed analysis\\nof chatbot performance based on both human and GPT-4 evaluations showing that\\nGPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Fur-\\nthermore, we find that current chatbot benchmarks are not trustworthy to accurately\\nevaluate the performance levels of chatbots. A lemon-picked analysis demonstrates\\nwhere Guanaco fails compared to ChatGPT. We release all of our models and code,\\nincluding CUDA kernels for 4-bit training.2\\n1 Introduction\\nFinetuning large language models (LLMs) is a highly effective way to improve their performance,\\n[40,63,43,62,60,37] and to add desirable or remove undesirable behaviors [ 43,2,4]. However,\\nfinetuning very large models is prohibitively expensive; regular 16-bit finetuning of a LLaMA 65B\\nparameter model [ 58] requires more than 780 GB of GPU memory. While recent quantization\\nmethods can reduce the memory footprint of LLMs [ 14,13,18,67], such techniques only work for\\ninference and break down during training [66].\\nWe demonstrate for the first time that it is possible to finetune a quantized 4-bit model without any\\nperformance degradation. Our method, QLORA, uses a novel high-precision technique to quantize\\na pretrained model to 4-bit, then adds a small set of learnable Low-rank Adapter weights [ 28]\\n∗Equal contribution.\\n2https://github.com/artidoro/qlora andhttps://github.com/TimDettmers/bitsandbytes\\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eb2e07a-e3c7-4d0f-96d9-e1a5b67cb4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79c23a32-195c-4468-9ee7-fb2ce3de256f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to c:\\users\\siddhanna.janai\\appdata\\local\\temp\\pip-req-build-uhpqy_h2\n",
      "  Resolved https://github.com/huggingface/transformers to commit 673d30b8268b584f9c600be27e048de0c714c0af\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: filelock in c:\\users\\siddhanna.janai\\appdata\\roaming\\python\\python311\\site-packages (from transformers==4.43.0.dev0) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\siddhanna.janai\\appdata\\roaming\\python\\python311\\site-packages (from transformers==4.43.0.dev0) (0.23.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\siddhanna.janai\\appdata\\roaming\\python\\python311\\site-packages (from transformers==4.43.0.dev0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\siddhanna.janai\\appdata\\roaming\\python\\python311\\site-packages (from transformers==4.43.0.dev0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\siddhanna.janai\\appdata\\roaming\\python\\python311\\site-packages (from transformers==4.43.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\siddhanna.janai\\appdata\\roaming\\python\\python311\\site-packages (from transformers==4.43.0.dev0) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\siddhanna.janai\\appdata\\roaming\\python\\python311\\site-packages (from transformers==4.43.0.dev0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\siddhanna.janai\\appdata\\roaming\\python\\python311\\site-packages (from transformers==4.43.0.dev0) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\siddhanna.janai\\appdata\\roaming\\python\\python311\\site-packages (from transformers==4.43.0.dev0) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\siddhanna.janai\\appdata\\roaming\\python\\python311\\site-packages (from transformers==4.43.0.dev0) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\siddhanna.janai\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.0.dev0) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\siddhanna.janai\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.0.dev0) (4.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\siddhanna.janai\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers==4.43.0.dev0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\siddhanna.janai\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers==4.43.0.dev0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\siddhanna.janai\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers==4.43.0.dev0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\siddhanna.janai\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers==4.43.0.dev0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\siddhanna.janai\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers==4.43.0.dev0) (2024.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers 'C:\\Users\\siddhanna.janai\\AppData\\Local\\Temp\\pip-req-build-uhpqy_h2'\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#%pip install git+https://github.com/huggingface/transformers\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fffebb9-96af-4482-bb74-5e5ec1b26d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "#from langchain import HuggingFaceHub\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "import os\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = getpass()\n",
    "#place your huggingface API key after running this cell\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efbb776c-bccc-4c4a-9d1e-53d434397108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddhanna.janai\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEndpoint`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"HuggingFaceH4/zephyr-7b-alpha\", #\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"top_k\": 30,\n",
    "        \"temperature\": 0.1,\n",
    "        \"repetition_penalty\": 1.03,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "582e2cd0-d73a-45c2-a053-0de93c218ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d299135e-25d6-448e-b6aa-612bb8ca9abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "def messages_to_prompt(messages):\n",
    "  prompt = \"\"\n",
    "  for message in messages:\n",
    "    if message.role == 'system':\n",
    "      prompt += f\"<|system|>\\n{message.content}</s>\\n\"\n",
    "    elif message.role == 'user':\n",
    "      prompt += f\"<|user|>\\n{message.content}</s>\\n\"\n",
    "    elif message.role == 'assistant':\n",
    "      prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"\n",
    "\n",
    "  # ensure we start with a system prompt, insert blank if needed\n",
    "  if not prompt.startswith(\"<|system|>\\n\"):\n",
    "    prompt = \"<|system|>\\n</s>\\n\" + prompt\n",
    "\n",
    "  # add final assistant prompt\n",
    "  prompt = prompt + \"<|assistant|>\\n\"\n",
    "\n",
    "  return prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b31d78f6-df2e-41c6-acc3-4b8da4a7845b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Google API: ········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "GOOGLE_API_KEY = getpass(\"Enter your Google API:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3798656-0ad6-49d5-a333-9da86ed40ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core import Settings\n",
    "import torch\n",
    "\n",
    "Settings.embed_model = GeminiEmbedding(model_name=\"models/embedding-001\", api_key=GOOGLE_API_KEY)\n",
    "Settings.llm = Gemini(model=\"models/gemini-1.5-pro\",api_key=GOOGLE_API_KEY,messages_to_prompt=messages_to_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc358138-8834-4e87-8aee-64c361858013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ServiceContext\n",
    "from llama_index.core import ServiceContext\n",
    "from llama_index.core import VectorStoreIndex, get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=Settings.llm,\n",
    "                                               embed_model=Settings.embed_model\n",
    "                                               )\n",
    "\n",
    "# index\n",
    "vector_index = VectorStoreIndex(\n",
    "    nodes, service_context=service_context\n",
    ")\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=vector_index,\n",
    "    similarity_top_k=5,\n",
    "    service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c7927e1-3bcb-4f71-a92c-baad1dcb6d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install llama-index \n",
    "#%pip install llama-index-postprocessor-cohere-rerank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d11bf25f-7692-4956-b6b0-30dd2ff6aef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all embeddings and rerankers\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "RERANKERS = {\n",
    "    \"WithoutReranker\": \"None\",\n",
    "    #\"CohereRerank\": CohereRerank(api_key=\"FmQZiK9b6m6Iypd7GGHvyi8oduAEuYvqsYNaBWf3\", top_n=5),\n",
    "    \"bge-reranker-base\": SentenceTransformerRerank(model=\"BAAI/bge-reranker-base\", top_n=5),\n",
    "    \"bge-reranker-large\": SentenceTransformerRerank(model=\"BAAI/bge-reranker-large\", top_n=5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "05705c5c-961b-4672-a799-18052b7fc7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "from copy import deepcopy\n",
    "from IPython.core.display import display, HTML\n",
    "import pandas as pd\n",
    "\n",
    "def get_retrieved_nodes(\n",
    "    query_str, reranker\n",
    "):\n",
    "    query_bundle = QueryBundle(query_str)\n",
    "\n",
    "    retrieved_nodes = retriever.retrieve(query_bundle)\n",
    "\n",
    "    if reranker != \"None\":\n",
    "      retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n",
    "    else:\n",
    "        retrieved_nodes\n",
    "\n",
    "    return retrieved_nodes\n",
    "\n",
    "\n",
    "def pretty_print(df):\n",
    "    return display(HTML(df.to_html().replace(\"\\\\n\", \"<br>\")))\n",
    "\n",
    "\n",
    "def visualize_retrieved_nodes(nodes) -> None:\n",
    "    result_dicts = []\n",
    "    for node in nodes:\n",
    "        node = deepcopy(node)\n",
    "        node.node.metadata = {}\n",
    "        node_text = node.node.get_text()\n",
    "        node_text = node_text.replace(\"\\n\", \" \")\n",
    "\n",
    "        result_dict = {\"Score\": node.score, \"Text\": node_text}\n",
    "        result_dicts.append(result_dict)\n",
    "\n",
    "    pretty_print(pd.DataFrame(result_dicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ade900f7-806d-4a24-81c4-776590d436ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install llama-index[query_tools]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "73be8c67-6c2b-4338-8414-bd5dfb16722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import QueryBundle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7356a838-884a-46b8-8a19-f596cf375791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Evaluation for Reranker: WithoutReranker\n",
      "Visualize Retrieved Nodes for Reranker: WithoutReranker\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.697117</td>\n",
       "      <td>[72] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y . Xu, W. Zheng, X. Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414 , 2022. [73] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V . Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 , 2022. [74] R. Zhong, K. Lee, Z. Zhang, and D. Klein. Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections. arXiv preprint arXiv:2104.04670 , 2021. 15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.693002</td>\n",
       "      <td>LLaMA model size0%25%50%75%100% 7B (6.9 GB) 13B (11.3 GB) 33B (24.7 GB) 65B (45.0 GB)Input gradient Optimizer Weight gradient Adapters ModelFigure 8: Breakdown of the memory footprint of different LLaMA models. The input gradient size is for batch size 1 and sequence length 512 and is estimated only for adapters and the base model weights (no attention). Numbers on the bars are memory footprint in MB of individual elements of the total footprint. While some models do not quite fit on certain GPUs, paged optimizers provide enough memory to allow these models to fit. 28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.677866</td>\n",
       "      <td>Chiang, Z. Li, Z. Lin, Y . Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y . Zhuang, J. E. Gonzalez, I. Stoica, and E. P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/ . [11] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems , 30, 2017. [12] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y . Tay, W. Fedus, E. Li, X. Wang, M. De- hghani, S. Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 , 2022. [13] T. Dettmers and L. Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. arXiv preprint arXiv:2212.09720 , 2022. [14] T. Dettmers, M. Lewis, Y . Belkada, and L. Zettlemoyer. LLM.int8(): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022 , 2022. [15] T. Dettmers, M. Lewis, S. Shleifer, and L. Zettlemoyer. 8-bit optimizers via block-wise quantization. 9th International Conference on Learning Representations, ICLR , 2022. [16] A. E. Elo. The proposed uscf rating system. its development, theory, and applications. Chess Life, 22(8):242–247, 1967. [17] A. E. Elo. The rating of chessplayers, past and present . Arco Pub., 1978. 11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.672513</td>\n",
       "      <td>Figure 1: Different finetuning methods and their memory requirements. QLORAimproves over LoRA by quantizing the transformer model to 4-bit precision and using paged optimizers to handle memory spikes. 2 Background Block-wise k-bit Quantization Quantization is the process of discretizing an input from a rep- resentation that holds more information to a representation with less information. It often means taking a data type with more bits and converting it to fewer bits, for example from 32-bit floats to 8-bit Integers. To ensure that the entire range of the low-bit data type is used, the input data type is commonly rescaled into the target data type range through normalization by the absolute maximum of the input elements, which are usually structured as a tensor. For example, quantizing a 32-bit Floating Point (FP32) tensor into a Int8 tensor with range [−127,127]: XInt8=round\u0012127 absmax (XFP32)XFP32\u0013 =round (cFP32·XFP32), (1) where cis the quantization constant orquantization scale . Dequantization is the inverse: dequant (cFP32,XInt8) =XInt8 cFP32=XFP32(2) The problem with this approach is that if a large magnitude value (i.e., an outlier) occurs in the input tensor, then the quantization bins—certain bit combinations—are not utilized well with few or no numbers quantized in some bins. To prevent the outlier issue, a common approach is to chunk the input tensor into blocks that are independently quantized, each with their own quantization constant c. This can be formalized as follows: We chunk the input tensor X∈Rb×hintoncontiguous blocks of sizeBby flattening the input tensor and slicing the linear segment into n= (b×h)/Bblocks. We quantize these blocks independently with Equation 1 to create a quantized tensor and nquantization constants ci. Low-rank Adapters Low-rank Adapter (LoRA) finetuning [ 28] is a method that reduces memory requirements by using a small set of trainable parameters, often termed adapters, while not updating the full model parameters which remain fixed. Gradients during stochastic gradient descent are passed through the fixed pretrained model weights to the adapter, which is updated to optimize the loss function. LoRA augments a linear projection through an additional factorized projection. Given a projection XW =YwithX∈Rb×h,W∈Rh×oLoRA computes: Y=XW +sXL 1L2, (3) whereL1∈Rh×randL2∈Rr×o, and sis a scalar. 3 QL ORA Finetuning QLORAachieves high-fidelity 4-bit finetuning via two techniques we propose—4-bit NormalFloat (NF4) quantization and Double Quantization. Additionally, we introduce Paged Optimizers, to 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.671981</td>\n",
       "      <td>[37] H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal, and C. A. Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems , 35:1950–1965, 2022. [38] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V . Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 , 2019. [39] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y . Tay, D. Zhou, Q. V . Le, B. Zoph, J. Wei, et al. The flan collection: Designing data and methods for effective instruction tuning. arXiv preprint arXiv:2301.13688 , 2023. [40] S. Min, M. Lewis, L. Zettlemoyer, and H. Hajishirzi. Metaicl: Learning to learn in context. arXiv preprint arXiv:2110.15943 , 2021. [41] A. Nematzadeh, K. Burns, E. Grant, A. Gopnik, and T. Griffiths. Evaluating theory of mind in question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 2392–2400, 2018. [42] OpenAI. Gpt-4 technical report. arXiv , 2023. [43] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems , 35:27730–27744, 2022. [44] G. Park, B. Park, S. J. Kwon, B. Kim, Y . Lee, and D. Lee. nuqmm: Quantized matmul for efficient inference of large-scale generative language models. arXiv preprint arXiv:2206.09557 , 2022. [45] B. Peng, C. Li, P. He, M. Galley, and J. Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277 , 2023. [46] F. Petersen and T. Sutter. Distributional Quantization. 2023. URL https://github.com/ Felix-Petersen/distquant . [47] A. Poliak, J. Naradowsky, A. Haldar, R. Rudinger, and B. Van Durme. Hypothesis only baselines in natural language inference. In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics , pages 180–191, 2018. [48] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, A. Levskaya, J. Heek, K. Xiao, S. Agrawal, and J. Dean. Efficiently scaling transformer inference. arXiv preprint arXiv:2211.05102 , 2022. [49] G. Qin and J. Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. arXiv preprint arXiv:2104.06599 , 2021. [50] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(1), jan 2020. ISSN 1532-4435. [51] V .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.669236</td>\n",
       "      <td>Table 4: Zero-shot Vicuna benchmark scores as a percentage of the score obtained by ChatGPT evaluated by GPT-4. We see that OASST1 models perform close to ChatGPT despite being trained on a very small dataset and having a fraction of the memory requirement of baseline models. Model / Dataset Params Model bits Memory ChatGPT vs Sys Sys vs ChatGPT Mean 95% CI GPT-4 - - - 119.4% 110.1% 114.5 % 2.6% Bard - - - 93.2% 96.4% 94.8% 4.1% Guanaco 65B 4-bit 41 GB 96.7% 101.9% 99.3% 4.4% Alpaca 65B 4-bit 41 GB 63.0% 77.9% 70.7% 4.3% FLAN v2 65B 4-bit 41 GB 37.0% 59.6% 48.4% 4.6% Guanaco 33B 4-bit 21 GB 96.5% 99.2% 97.8% 4.4% Open Assistant 33B 16-bit 66 GB 73.4% 85.7% 78.1% 5.3% Alpaca 33B 4-bit 21 GB 67.2% 79.7% 73.6% 4.2% FLAN v2 33B 4-bit 21 GB 26.3% 49.7% 38.0% 3.9% Vicuna 13B 16-bit 26 GB 91.2% 98.7% 94.9% 4.5% Guanaco 13B 4-bit 10 GB 87.3% 93.4% 90.4% 5.2% Alpaca 13B 4-bit 10 GB 63.8% 76.7% 69.4% 4.2% HH-RLHF 13B 4-bit 10 GB 55.5% 69.1% 62.5% 4.7% Unnatural Instr. 13B 4-bit 10 GB 50.6% 69.8% 60.5% 4.2% Chip2 13B 4-bit 10 GB 49.2% 69.3% 59.5% 4.7% Longform 13B 4-bit 10 GB 44.9% 62.0% 53.6% 5.2% Self-Instruct 13B 4-bit 10 GB 38.0% 60.5% 49.1% 4.6% FLAN v2 13B 4-bit 10 GB 32.4% 61.2% 47.0% 3.6% Guanaco 7B 4-bit 5 GB 84.1% 89.8% 87.0% 5.4% Alpaca 7B 4-bit 5 GB 57.3% 71.2% 64.4% 5.0% FLAN v2 7B 4-bit 5 GB 33.3% 56.1% 44.8% 4.0% Table 5: MMLU 5-shot test results for different sizes of LLaMA finetuned on the corresponding datasets using QLoRA.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.667859</td>\n",
       "      <td>Opt-iml: Scaling language model instruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017 , 2022. [30] A. Köksal, T. Schick, A. Korhonen, and H. Schütze. Longform: Optimizing instruction tuning for long text generation with corpus extraction. arXiv preprint arXiv:2304.08460 , 2023. [31] A. Köpf, Y . Kilcher, D. von Rütte, S. Anagnostidis, Z.-R. Tam, K. Stevens, A. Barhoum, N. M. Duc, O. Stanley, R. Nagyfi, et al. Openassistant conversations–democratizing large language model alignment. arXiv preprint arXiv:2304.07327 , 2023. [32] LAION. Open-instruction-generalist dataset. https://github.com/LAION-AI/ Open-Instruction-Generalist , 2023. [33] B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691 , 2021. [34] X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190 , 2021. [35] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y . Zhang, D. Narayanan, Y . Wu, A. Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110 , 2022. [36] T. Liao, R. Taori, I. D. Raji, and L. Schmidt. Are we learning yet? a meta review of evaluation failures across machine learning. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2) , 2021. 12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.663670</td>\n",
       "      <td>The main question now is whether QLoRA can perform as well as full-model finetuning. Furthermore, we want to analyze the components of QLoRA including the impact of NormalFloat4 over standard Float4. The follow- ing sections will discuss the experiments that aimed at answering these questions. Experimental setup. We consider three architec- tures (encoder, encoder-decoder, and decoder only) and compare QLoRA with 16-bit adapter-finetuning and with full-finetuning for models up to 3B. Our evaluations include GLUE [ 59] with RoBERTa- large [ 38], Super-NaturalInstructionss [ 62] with T5 [50], and 5-shot MMLU [ 24] after finetuning LLaMA on Flan v2 [39] and Alpaca [56]. To additionally study the advantages of NF4 over other 4-bit data types, we use the setup of Dettmers and Zettlemoyer [13] and measure post-quantization zero-shot accuracy and perplexity across different models (OPT [ 73], LLaMA [ 58], BLOOM [ 53], Pythia [ 7]) for model sizes 125m - 13B. We provide more details in the results section for each particular setup to make the results more readable. Full details in Appendix C. While paged optimizers are critical to do 33B/65B QLORAtuning on a single 24/48GB GPU, we do not provide hard measurements for Paged Optimizers since the paging only occurs when processing 3https://docs.nvidia.com/cuda/cuda-c-programming-guide 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.658929</td>\n",
       "      <td>quantization. On average, for a blocksize of 64, this quantization reduces the memory footprint per parameter from 32/64 = 0 .5bits, to 8/64 + 32 /(64·256) = 0 .127bits, a reduction of 0.373 bits per parameter. Paged Optimizers use the NVIDIA unified memory3feature which does automatic page-to-page transfers between the CPU and GPU for error-free GPU processing in the scenario where the GPU occasionally runs out-of-memory. The feature works like regular memory paging between CPU RAM and the disk. We use this feature to allocate paged memory for the optimizer states which are then automatically evicted to CPU RAM when the GPU runs out-of-memory and paged back into GPU memory when the memory is needed in the optimizer update step. QL ORA.Using the components described above, we define QLORAfor a single linear layer in the quantized base model with a single LoRA adapter as follows: YBF16=XBF16doubleDequant (cFP32 1, ck-bit 2,WNF4) +XBF16LBF16 1LBF16 2, (5) doubleDequant (cFP32 1, ck-bit 2,Wk-bit) =dequant (dequant (cFP32 1, ck-bit 2),W4bit) =WBF16,(6) We use NF4 for Wand FP8 for c2. We use a blocksize of 64 for Wfor higher quantization precision and a blocksize of 256 for c2to conserve memory. For parameter updates only the gradient with respect to the error for the adapters weights∂E ∂Liare needed, and not for 4-bit weights∂E ∂W. However, the calculation of∂E ∂Lientails the calculation of∂X ∂W which proceeds via equation (5) with dequantization from storage WNF4to computation data type WBF16to calculate the derivative∂X ∂Win BFloat16 precision. To summarize, QLORAhas one storage data type (usually 4-bit NormalFloat) and a computation data type (16-bit BrainFloat). We dequantize the storage data type to the computation data type to perform the forward and backward pass, but we only compute weight gradients for the LoRA parameters which use 16-bit BrainFloat. 4 QLoRA vs. Standard Finetuning 1010 1011 T otal model bits 0.60 0.61 0.62 0.63 0.64 0.65 0.66 0.67Mean zeroshot accuracy 4-bit LLaMA Float NFloat NFloat + DQData type Figure 2: Mean zero-shot accuracy over Wino- grande, HellaSwag, PiQA, Arc-Easy, and Arc- Challenge using LLaMA models with different 4-bit data types. The NormalFloat data type significantly improves the bit-for-bit accuracy gains compared to regular 4-bit Floats. While Double Quantization (DQ) only leads to minor gains, it allows for a more fine-grained control over the memory footprint to fit models of certain size (33B/65B) into certain GPUs (24/48GB).We have discussed how QLoRA works and how it can significantly reduce the required memory for finetuning models. The main question now is whether QLoRA can perform as well as full-model finetuning. Furthermore, we want to analyze the components of QLoRA including the impact of NormalFloat4 over standard Float4. The follow- ing sections will discuss the experiments that aimed at answering these questions. Experimental setup. We consider three architec- tures (encoder, encoder-decoder, and decoder only) and compare QLoRA with 16-bit adapter-finetuning and with full-finetuning for models up to 3B. Our evaluations include GLUE [ 59] with RoBERTa- large [ 38], Super-NaturalInstructionss [ 62] with T5 [50], and 5-shot MMLU [ 24] after finetuning LLaMA on Flan v2 [39] and Alpaca [56].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.653471</td>\n",
       "      <td>All in all, we believe that QLORAwill have a broadly positive impact making the finetuning of high quality LLMs much more widely and easily accessible. Acknowledgements We thank Aditya Kusupati, Ofir Press, Ashish Sharma, Margaret Li, Raphael Olivier, Zihao Ye, and Evangelia Spiliopoulou for their valuable feedback. We thank Norah Altriri for providing us with an intuitive NF4 visualization. We thank Felix Petersen for a discussion on distributional quantization and its relationship to quantile quantization. Our research was facilitated by the advanced computational, storage, and networking infrastructure of the Hyak supercomputer system at the University of Washington. We thank the Hyak team for ensuring a smooth operation. We thank the beta testers of the bitsandbytes library, in particular Alex Birch and Alyssa Vance. We thank Younes Belkada for help with the integration of our software into the Hugging Face transformers stack. 10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Evaluation for Reranker: CohereRerank\n",
      "Visualize Retrieved Nodes for Reranker: CohereRerank\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.998545</td>\n",
       "      <td>Figure 1: Different finetuning methods and their memory requirements. QLORAimproves over LoRA by quantizing the transformer model to 4-bit precision and using paged optimizers to handle memory spikes. 2 Background Block-wise k-bit Quantization Quantization is the process of discretizing an input from a rep- resentation that holds more information to a representation with less information. It often means taking a data type with more bits and converting it to fewer bits, for example from 32-bit floats to 8-bit Integers. To ensure that the entire range of the low-bit data type is used, the input data type is commonly rescaled into the target data type range through normalization by the absolute maximum of the input elements, which are usually structured as a tensor. For example, quantizing a 32-bit Floating Point (FP32) tensor into a Int8 tensor with range [−127,127]: XInt8=round\u0012127 absmax (XFP32)XFP32\u0013 =round (cFP32·XFP32), (1) where cis the quantization constant orquantization scale . Dequantization is the inverse: dequant (cFP32,XInt8) =XInt8 cFP32=XFP32(2) The problem with this approach is that if a large magnitude value (i.e., an outlier) occurs in the input tensor, then the quantization bins—certain bit combinations—are not utilized well with few or no numbers quantized in some bins. To prevent the outlier issue, a common approach is to chunk the input tensor into blocks that are independently quantized, each with their own quantization constant c. This can be formalized as follows: We chunk the input tensor X∈Rb×hintoncontiguous blocks of sizeBby flattening the input tensor and slicing the linear segment into n= (b×h)/Bblocks. We quantize these blocks independently with Equation 1 to create a quantized tensor and nquantization constants ci. Low-rank Adapters Low-rank Adapter (LoRA) finetuning [ 28] is a method that reduces memory requirements by using a small set of trainable parameters, often termed adapters, while not updating the full model parameters which remain fixed. Gradients during stochastic gradient descent are passed through the fixed pretrained model weights to the adapter, which is updated to optimize the loss function. LoRA augments a linear projection through an additional factorized projection. Given a projection XW =YwithX∈Rb×h,W∈Rh×oLoRA computes: Y=XW +sXL 1L2, (3) whereL1∈Rh×randL2∈Rr×o, and sis a scalar. 3 QL ORA Finetuning QLORAachieves high-fidelity 4-bit finetuning via two techniques we propose—4-bit NormalFloat (NF4) quantization and Double Quantization. Additionally, we introduce Paged Optimizers, to 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.947769</td>\n",
       "      <td>quantization. On average, for a blocksize of 64, this quantization reduces the memory footprint per parameter from 32/64 = 0 .5bits, to 8/64 + 32 /(64·256) = 0 .127bits, a reduction of 0.373 bits per parameter. Paged Optimizers use the NVIDIA unified memory3feature which does automatic page-to-page transfers between the CPU and GPU for error-free GPU processing in the scenario where the GPU occasionally runs out-of-memory. The feature works like regular memory paging between CPU RAM and the disk. We use this feature to allocate paged memory for the optimizer states which are then automatically evicted to CPU RAM when the GPU runs out-of-memory and paged back into GPU memory when the memory is needed in the optimizer update step. QL ORA.Using the components described above, we define QLORAfor a single linear layer in the quantized base model with a single LoRA adapter as follows: YBF16=XBF16doubleDequant (cFP32 1, ck-bit 2,WNF4) +XBF16LBF16 1LBF16 2, (5) doubleDequant (cFP32 1, ck-bit 2,Wk-bit) =dequant (dequant (cFP32 1, ck-bit 2),W4bit) =WBF16,(6) We use NF4 for Wand FP8 for c2. We use a blocksize of 64 for Wfor higher quantization precision and a blocksize of 256 for c2to conserve memory. For parameter updates only the gradient with respect to the error for the adapters weights∂E ∂Liare needed, and not for 4-bit weights∂E ∂W. However, the calculation of∂E ∂Lientails the calculation of∂X ∂W which proceeds via equation (5) with dequantization from storage WNF4to computation data type WBF16to calculate the derivative∂X ∂Win BFloat16 precision. To summarize, QLORAhas one storage data type (usually 4-bit NormalFloat) and a computation data type (16-bit BrainFloat). We dequantize the storage data type to the computation data type to perform the forward and backward pass, but we only compute weight gradients for the LoRA parameters which use 16-bit BrainFloat. 4 QLoRA vs. Standard Finetuning 1010 1011 T otal model bits 0.60 0.61 0.62 0.63 0.64 0.65 0.66 0.67Mean zeroshot accuracy 4-bit LLaMA Float NFloat NFloat + DQData type Figure 2: Mean zero-shot accuracy over Wino- grande, HellaSwag, PiQA, Arc-Easy, and Arc- Challenge using LLaMA models with different 4-bit data types. The NormalFloat data type significantly improves the bit-for-bit accuracy gains compared to regular 4-bit Floats. While Double Quantization (DQ) only leads to minor gains, it allows for a more fine-grained control over the memory footprint to fit models of certain size (33B/65B) into certain GPUs (24/48GB).We have discussed how QLoRA works and how it can significantly reduce the required memory for finetuning models. The main question now is whether QLoRA can perform as well as full-model finetuning. Furthermore, we want to analyze the components of QLoRA including the impact of NormalFloat4 over standard Float4. The follow- ing sections will discuss the experiments that aimed at answering these questions. Experimental setup. We consider three architec- tures (encoder, encoder-decoder, and decoder only) and compare QLoRA with 16-bit adapter-finetuning and with full-finetuning for models up to 3B. Our evaluations include GLUE [ 59] with RoBERTa- large [ 38], Super-NaturalInstructionss [ 62] with T5 [50], and 5-shot MMLU [ 24] after finetuning LLaMA on Flan v2 [39] and Alpaca [56].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.597079</td>\n",
       "      <td>Table 4: Zero-shot Vicuna benchmark scores as a percentage of the score obtained by ChatGPT evaluated by GPT-4. We see that OASST1 models perform close to ChatGPT despite being trained on a very small dataset and having a fraction of the memory requirement of baseline models. Model / Dataset Params Model bits Memory ChatGPT vs Sys Sys vs ChatGPT Mean 95% CI GPT-4 - - - 119.4% 110.1% 114.5 % 2.6% Bard - - - 93.2% 96.4% 94.8% 4.1% Guanaco 65B 4-bit 41 GB 96.7% 101.9% 99.3% 4.4% Alpaca 65B 4-bit 41 GB 63.0% 77.9% 70.7% 4.3% FLAN v2 65B 4-bit 41 GB 37.0% 59.6% 48.4% 4.6% Guanaco 33B 4-bit 21 GB 96.5% 99.2% 97.8% 4.4% Open Assistant 33B 16-bit 66 GB 73.4% 85.7% 78.1% 5.3% Alpaca 33B 4-bit 21 GB 67.2% 79.7% 73.6% 4.2% FLAN v2 33B 4-bit 21 GB 26.3% 49.7% 38.0% 3.9% Vicuna 13B 16-bit 26 GB 91.2% 98.7% 94.9% 4.5% Guanaco 13B 4-bit 10 GB 87.3% 93.4% 90.4% 5.2% Alpaca 13B 4-bit 10 GB 63.8% 76.7% 69.4% 4.2% HH-RLHF 13B 4-bit 10 GB 55.5% 69.1% 62.5% 4.7% Unnatural Instr. 13B 4-bit 10 GB 50.6% 69.8% 60.5% 4.2% Chip2 13B 4-bit 10 GB 49.2% 69.3% 59.5% 4.7% Longform 13B 4-bit 10 GB 44.9% 62.0% 53.6% 5.2% Self-Instruct 13B 4-bit 10 GB 38.0% 60.5% 49.1% 4.6% FLAN v2 13B 4-bit 10 GB 32.4% 61.2% 47.0% 3.6% Guanaco 7B 4-bit 5 GB 84.1% 89.8% 87.0% 5.4% Alpaca 7B 4-bit 5 GB 57.3% 71.2% 64.4% 5.0% FLAN v2 7B 4-bit 5 GB 33.3% 56.1% 44.8% 4.0% Table 5: MMLU 5-shot test results for different sizes of LLaMA finetuned on the corresponding datasets using QLoRA.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.320928</td>\n",
       "      <td>All in all, we believe that QLORAwill have a broadly positive impact making the finetuning of high quality LLMs much more widely and easily accessible. Acknowledgements We thank Aditya Kusupati, Ofir Press, Ashish Sharma, Margaret Li, Raphael Olivier, Zihao Ye, and Evangelia Spiliopoulou for their valuable feedback. We thank Norah Altriri for providing us with an intuitive NF4 visualization. We thank Felix Petersen for a discussion on distributional quantization and its relationship to quantile quantization. Our research was facilitated by the advanced computational, storage, and networking infrastructure of the Hyak supercomputer system at the University of Washington. We thank the Hyak team for ensuring a smooth operation. We thank the beta testers of the bitsandbytes library, in particular Alex Birch and Alyssa Vance. We thank Younes Belkada for help with the integration of our software into the Hugging Face transformers stack. 10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.278257</td>\n",
       "      <td>The main question now is whether QLoRA can perform as well as full-model finetuning. Furthermore, we want to analyze the components of QLoRA including the impact of NormalFloat4 over standard Float4. The follow- ing sections will discuss the experiments that aimed at answering these questions. Experimental setup. We consider three architec- tures (encoder, encoder-decoder, and decoder only) and compare QLoRA with 16-bit adapter-finetuning and with full-finetuning for models up to 3B. Our evaluations include GLUE [ 59] with RoBERTa- large [ 38], Super-NaturalInstructionss [ 62] with T5 [50], and 5-shot MMLU [ 24] after finetuning LLaMA on Flan v2 [39] and Alpaca [56]. To additionally study the advantages of NF4 over other 4-bit data types, we use the setup of Dettmers and Zettlemoyer [13] and measure post-quantization zero-shot accuracy and perplexity across different models (OPT [ 73], LLaMA [ 58], BLOOM [ 53], Pythia [ 7]) for model sizes 125m - 13B. We provide more details in the results section for each particular setup to make the results more readable. Full details in Appendix C. While paged optimizers are critical to do 33B/65B QLORAtuning on a single 24/48GB GPU, we do not provide hard measurements for Paged Optimizers since the paging only occurs when processing 3https://docs.nvidia.com/cuda/cuda-c-programming-guide 5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Evaluation for Reranker: bge-reranker-base\n",
      "Visualize Retrieved Nodes for Reranker: bge-reranker-base\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.998165</td>\n",
       "      <td>Figure 1: Different finetuning methods and their memory requirements. QLORAimproves over LoRA by quantizing the transformer model to 4-bit precision and using paged optimizers to handle memory spikes. 2 Background Block-wise k-bit Quantization Quantization is the process of discretizing an input from a rep- resentation that holds more information to a representation with less information. It often means taking a data type with more bits and converting it to fewer bits, for example from 32-bit floats to 8-bit Integers. To ensure that the entire range of the low-bit data type is used, the input data type is commonly rescaled into the target data type range through normalization by the absolute maximum of the input elements, which are usually structured as a tensor. For example, quantizing a 32-bit Floating Point (FP32) tensor into a Int8 tensor with range [−127,127]: XInt8=round\u0012127 absmax (XFP32)XFP32\u0013 =round (cFP32·XFP32), (1) where cis the quantization constant orquantization scale . Dequantization is the inverse: dequant (cFP32,XInt8) =XInt8 cFP32=XFP32(2) The problem with this approach is that if a large magnitude value (i.e., an outlier) occurs in the input tensor, then the quantization bins—certain bit combinations—are not utilized well with few or no numbers quantized in some bins. To prevent the outlier issue, a common approach is to chunk the input tensor into blocks that are independently quantized, each with their own quantization constant c. This can be formalized as follows: We chunk the input tensor X∈Rb×hintoncontiguous blocks of sizeBby flattening the input tensor and slicing the linear segment into n= (b×h)/Bblocks. We quantize these blocks independently with Equation 1 to create a quantized tensor and nquantization constants ci. Low-rank Adapters Low-rank Adapter (LoRA) finetuning [ 28] is a method that reduces memory requirements by using a small set of trainable parameters, often termed adapters, while not updating the full model parameters which remain fixed. Gradients during stochastic gradient descent are passed through the fixed pretrained model weights to the adapter, which is updated to optimize the loss function. LoRA augments a linear projection through an additional factorized projection. Given a projection XW =YwithX∈Rb×h,W∈Rh×oLoRA computes: Y=XW +sXL 1L2, (3) whereL1∈Rh×randL2∈Rr×o, and sis a scalar. 3 QL ORA Finetuning QLORAachieves high-fidelity 4-bit finetuning via two techniques we propose—4-bit NormalFloat (NF4) quantization and Double Quantization. Additionally, we introduce Paged Optimizers, to 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.066596</td>\n",
       "      <td>The main question now is whether QLoRA can perform as well as full-model finetuning. Furthermore, we want to analyze the components of QLoRA including the impact of NormalFloat4 over standard Float4. The follow- ing sections will discuss the experiments that aimed at answering these questions. Experimental setup. We consider three architec- tures (encoder, encoder-decoder, and decoder only) and compare QLoRA with 16-bit adapter-finetuning and with full-finetuning for models up to 3B. Our evaluations include GLUE [ 59] with RoBERTa- large [ 38], Super-NaturalInstructionss [ 62] with T5 [50], and 5-shot MMLU [ 24] after finetuning LLaMA on Flan v2 [39] and Alpaca [56]. To additionally study the advantages of NF4 over other 4-bit data types, we use the setup of Dettmers and Zettlemoyer [13] and measure post-quantization zero-shot accuracy and perplexity across different models (OPT [ 73], LLaMA [ 58], BLOOM [ 53], Pythia [ 7]) for model sizes 125m - 13B. We provide more details in the results section for each particular setup to make the results more readable. Full details in Appendix C. While paged optimizers are critical to do 33B/65B QLORAtuning on a single 24/48GB GPU, we do not provide hard measurements for Paged Optimizers since the paging only occurs when processing 3https://docs.nvidia.com/cuda/cuda-c-programming-guide 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.056656</td>\n",
       "      <td>[37] H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal, and C. A. Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems , 35:1950–1965, 2022. [38] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V . Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 , 2019. [39] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y . Tay, D. Zhou, Q. V . Le, B. Zoph, J. Wei, et al. The flan collection: Designing data and methods for effective instruction tuning. arXiv preprint arXiv:2301.13688 , 2023. [40] S. Min, M. Lewis, L. Zettlemoyer, and H. Hajishirzi. Metaicl: Learning to learn in context. arXiv preprint arXiv:2110.15943 , 2021. [41] A. Nematzadeh, K. Burns, E. Grant, A. Gopnik, and T. Griffiths. Evaluating theory of mind in question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 2392–2400, 2018. [42] OpenAI. Gpt-4 technical report. arXiv , 2023. [43] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems , 35:27730–27744, 2022. [44] G. Park, B. Park, S. J. Kwon, B. Kim, Y . Lee, and D. Lee. nuqmm: Quantized matmul for efficient inference of large-scale generative language models. arXiv preprint arXiv:2206.09557 , 2022. [45] B. Peng, C. Li, P. He, M. Galley, and J. Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277 , 2023. [46] F. Petersen and T. Sutter. Distributional Quantization. 2023. URL https://github.com/ Felix-Petersen/distquant . [47] A. Poliak, J. Naradowsky, A. Haldar, R. Rudinger, and B. Van Durme. Hypothesis only baselines in natural language inference. In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics , pages 180–191, 2018. [48] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, A. Levskaya, J. Heek, K. Xiao, S. Agrawal, and J. Dean. Efficiently scaling transformer inference. arXiv preprint arXiv:2211.05102 , 2022. [49] G. Qin and J. Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. arXiv preprint arXiv:2104.06599 , 2021. [50] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(1), jan 2020. ISSN 1532-4435. [51] V .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.020892</td>\n",
       "      <td>Table 4: Zero-shot Vicuna benchmark scores as a percentage of the score obtained by ChatGPT evaluated by GPT-4. We see that OASST1 models perform close to ChatGPT despite being trained on a very small dataset and having a fraction of the memory requirement of baseline models. Model / Dataset Params Model bits Memory ChatGPT vs Sys Sys vs ChatGPT Mean 95% CI GPT-4 - - - 119.4% 110.1% 114.5 % 2.6% Bard - - - 93.2% 96.4% 94.8% 4.1% Guanaco 65B 4-bit 41 GB 96.7% 101.9% 99.3% 4.4% Alpaca 65B 4-bit 41 GB 63.0% 77.9% 70.7% 4.3% FLAN v2 65B 4-bit 41 GB 37.0% 59.6% 48.4% 4.6% Guanaco 33B 4-bit 21 GB 96.5% 99.2% 97.8% 4.4% Open Assistant 33B 16-bit 66 GB 73.4% 85.7% 78.1% 5.3% Alpaca 33B 4-bit 21 GB 67.2% 79.7% 73.6% 4.2% FLAN v2 33B 4-bit 21 GB 26.3% 49.7% 38.0% 3.9% Vicuna 13B 16-bit 26 GB 91.2% 98.7% 94.9% 4.5% Guanaco 13B 4-bit 10 GB 87.3% 93.4% 90.4% 5.2% Alpaca 13B 4-bit 10 GB 63.8% 76.7% 69.4% 4.2% HH-RLHF 13B 4-bit 10 GB 55.5% 69.1% 62.5% 4.7% Unnatural Instr. 13B 4-bit 10 GB 50.6% 69.8% 60.5% 4.2% Chip2 13B 4-bit 10 GB 49.2% 69.3% 59.5% 4.7% Longform 13B 4-bit 10 GB 44.9% 62.0% 53.6% 5.2% Self-Instruct 13B 4-bit 10 GB 38.0% 60.5% 49.1% 4.6% FLAN v2 13B 4-bit 10 GB 32.4% 61.2% 47.0% 3.6% Guanaco 7B 4-bit 5 GB 84.1% 89.8% 87.0% 5.4% Alpaca 7B 4-bit 5 GB 57.3% 71.2% 64.4% 5.0% FLAN v2 7B 4-bit 5 GB 33.3% 56.1% 44.8% 4.0% Table 5: MMLU 5-shot test results for different sizes of LLaMA finetuned on the corresponding datasets using QLoRA.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.014146</td>\n",
       "      <td>All in all, we believe that QLORAwill have a broadly positive impact making the finetuning of high quality LLMs much more widely and easily accessible. Acknowledgements We thank Aditya Kusupati, Ofir Press, Ashish Sharma, Margaret Li, Raphael Olivier, Zihao Ye, and Evangelia Spiliopoulou for their valuable feedback. We thank Norah Altriri for providing us with an intuitive NF4 visualization. We thank Felix Petersen for a discussion on distributional quantization and its relationship to quantile quantization. Our research was facilitated by the advanced computational, storage, and networking infrastructure of the Hyak supercomputer system at the University of Washington. We thank the Hyak team for ensuring a smooth operation. We thank the beta testers of the bitsandbytes library, in particular Alex Birch and Alyssa Vance. We thank Younes Belkada for help with the integration of our software into the Hugging Face transformers stack. 10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Evaluation for Reranker: bge-reranker-large\n",
      "Visualize Retrieved Nodes for Reranker: bge-reranker-large\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.995404</td>\n",
       "      <td>Figure 1: Different finetuning methods and their memory requirements. QLORAimproves over LoRA by quantizing the transformer model to 4-bit precision and using paged optimizers to handle memory spikes. 2 Background Block-wise k-bit Quantization Quantization is the process of discretizing an input from a rep- resentation that holds more information to a representation with less information. It often means taking a data type with more bits and converting it to fewer bits, for example from 32-bit floats to 8-bit Integers. To ensure that the entire range of the low-bit data type is used, the input data type is commonly rescaled into the target data type range through normalization by the absolute maximum of the input elements, which are usually structured as a tensor. For example, quantizing a 32-bit Floating Point (FP32) tensor into a Int8 tensor with range [−127,127]: XInt8=round\u0012127 absmax (XFP32)XFP32\u0013 =round (cFP32·XFP32), (1) where cis the quantization constant orquantization scale . Dequantization is the inverse: dequant (cFP32,XInt8) =XInt8 cFP32=XFP32(2) The problem with this approach is that if a large magnitude value (i.e., an outlier) occurs in the input tensor, then the quantization bins—certain bit combinations—are not utilized well with few or no numbers quantized in some bins. To prevent the outlier issue, a common approach is to chunk the input tensor into blocks that are independently quantized, each with their own quantization constant c. This can be formalized as follows: We chunk the input tensor X∈Rb×hintoncontiguous blocks of sizeBby flattening the input tensor and slicing the linear segment into n= (b×h)/Bblocks. We quantize these blocks independently with Equation 1 to create a quantized tensor and nquantization constants ci. Low-rank Adapters Low-rank Adapter (LoRA) finetuning [ 28] is a method that reduces memory requirements by using a small set of trainable parameters, often termed adapters, while not updating the full model parameters which remain fixed. Gradients during stochastic gradient descent are passed through the fixed pretrained model weights to the adapter, which is updated to optimize the loss function. LoRA augments a linear projection through an additional factorized projection. Given a projection XW =YwithX∈Rb×h,W∈Rh×oLoRA computes: Y=XW +sXL 1L2, (3) whereL1∈Rh×randL2∈Rr×o, and sis a scalar. 3 QL ORA Finetuning QLORAachieves high-fidelity 4-bit finetuning via two techniques we propose—4-bit NormalFloat (NF4) quantization and Double Quantization. Additionally, we introduce Paged Optimizers, to 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.042283</td>\n",
       "      <td>The main question now is whether QLoRA can perform as well as full-model finetuning. Furthermore, we want to analyze the components of QLoRA including the impact of NormalFloat4 over standard Float4. The follow- ing sections will discuss the experiments that aimed at answering these questions. Experimental setup. We consider three architec- tures (encoder, encoder-decoder, and decoder only) and compare QLoRA with 16-bit adapter-finetuning and with full-finetuning for models up to 3B. Our evaluations include GLUE [ 59] with RoBERTa- large [ 38], Super-NaturalInstructionss [ 62] with T5 [50], and 5-shot MMLU [ 24] after finetuning LLaMA on Flan v2 [39] and Alpaca [56]. To additionally study the advantages of NF4 over other 4-bit data types, we use the setup of Dettmers and Zettlemoyer [13] and measure post-quantization zero-shot accuracy and perplexity across different models (OPT [ 73], LLaMA [ 58], BLOOM [ 53], Pythia [ 7]) for model sizes 125m - 13B. We provide more details in the results section for each particular setup to make the results more readable. Full details in Appendix C. While paged optimizers are critical to do 33B/65B QLORAtuning on a single 24/48GB GPU, we do not provide hard measurements for Paged Optimizers since the paging only occurs when processing 3https://docs.nvidia.com/cuda/cuda-c-programming-guide 5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.035546</td>\n",
       "      <td>Table 4: Zero-shot Vicuna benchmark scores as a percentage of the score obtained by ChatGPT evaluated by GPT-4. We see that OASST1 models perform close to ChatGPT despite being trained on a very small dataset and having a fraction of the memory requirement of baseline models. Model / Dataset Params Model bits Memory ChatGPT vs Sys Sys vs ChatGPT Mean 95% CI GPT-4 - - - 119.4% 110.1% 114.5 % 2.6% Bard - - - 93.2% 96.4% 94.8% 4.1% Guanaco 65B 4-bit 41 GB 96.7% 101.9% 99.3% 4.4% Alpaca 65B 4-bit 41 GB 63.0% 77.9% 70.7% 4.3% FLAN v2 65B 4-bit 41 GB 37.0% 59.6% 48.4% 4.6% Guanaco 33B 4-bit 21 GB 96.5% 99.2% 97.8% 4.4% Open Assistant 33B 16-bit 66 GB 73.4% 85.7% 78.1% 5.3% Alpaca 33B 4-bit 21 GB 67.2% 79.7% 73.6% 4.2% FLAN v2 33B 4-bit 21 GB 26.3% 49.7% 38.0% 3.9% Vicuna 13B 16-bit 26 GB 91.2% 98.7% 94.9% 4.5% Guanaco 13B 4-bit 10 GB 87.3% 93.4% 90.4% 5.2% Alpaca 13B 4-bit 10 GB 63.8% 76.7% 69.4% 4.2% HH-RLHF 13B 4-bit 10 GB 55.5% 69.1% 62.5% 4.7% Unnatural Instr. 13B 4-bit 10 GB 50.6% 69.8% 60.5% 4.2% Chip2 13B 4-bit 10 GB 49.2% 69.3% 59.5% 4.7% Longform 13B 4-bit 10 GB 44.9% 62.0% 53.6% 5.2% Self-Instruct 13B 4-bit 10 GB 38.0% 60.5% 49.1% 4.6% FLAN v2 13B 4-bit 10 GB 32.4% 61.2% 47.0% 3.6% Guanaco 7B 4-bit 5 GB 84.1% 89.8% 87.0% 5.4% Alpaca 7B 4-bit 5 GB 57.3% 71.2% 64.4% 5.0% FLAN v2 7B 4-bit 5 GB 33.3% 56.1% 44.8% 4.0% Table 5: MMLU 5-shot test results for different sizes of LLaMA finetuned on the corresponding datasets using QLoRA.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.022065</td>\n",
       "      <td>[37] H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal, and C. A. Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems , 35:1950–1965, 2022. [38] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V . Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 , 2019. [39] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y . Tay, D. Zhou, Q. V . Le, B. Zoph, J. Wei, et al. The flan collection: Designing data and methods for effective instruction tuning. arXiv preprint arXiv:2301.13688 , 2023. [40] S. Min, M. Lewis, L. Zettlemoyer, and H. Hajishirzi. Metaicl: Learning to learn in context. arXiv preprint arXiv:2110.15943 , 2021. [41] A. Nematzadeh, K. Burns, E. Grant, A. Gopnik, and T. Griffiths. Evaluating theory of mind in question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 2392–2400, 2018. [42] OpenAI. Gpt-4 technical report. arXiv , 2023. [43] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems , 35:27730–27744, 2022. [44] G. Park, B. Park, S. J. Kwon, B. Kim, Y . Lee, and D. Lee. nuqmm: Quantized matmul for efficient inference of large-scale generative language models. arXiv preprint arXiv:2206.09557 , 2022. [45] B. Peng, C. Li, P. He, M. Galley, and J. Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277 , 2023. [46] F. Petersen and T. Sutter. Distributional Quantization. 2023. URL https://github.com/ Felix-Petersen/distquant . [47] A. Poliak, J. Naradowsky, A. Haldar, R. Rudinger, and B. Van Durme. Hypothesis only baselines in natural language inference. In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics , pages 180–191, 2018. [48] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, A. Levskaya, J. Heek, K. Xiao, S. Agrawal, and J. Dean. Efficiently scaling transformer inference. arXiv preprint arXiv:2211.05102 , 2022. [49] G. Qin and J. Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. arXiv preprint arXiv:2104.06599 , 2021. [50] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(1), jan 2020. ISSN 1532-4435. [51] V .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.016779</td>\n",
       "      <td>LLaMA model size0%25%50%75%100% 7B (6.9 GB) 13B (11.3 GB) 33B (24.7 GB) 65B (45.0 GB)Input gradient Optimizer Weight gradient Adapters ModelFigure 8: Breakdown of the memory footprint of different LLaMA models. The input gradient size is for batch size 1 and sequence length 512 and is estimated only for adapters and the base model weights (no attention). Numbers on the bars are memory footprint in MB of individual elements of the total footprint. While some models do not quite fit on certain GPUs, paged optimizers provide enough memory to allow these models to fit. 28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_str = \"Different finetuning methods and their memory requirements\"\n",
    "\n",
    "# Loop over rerankers\n",
    "for rerank_name, reranker in RERANKERS.items():\n",
    "    print(f\"Running Evaluation for Reranker: {rerank_name}\")\n",
    "\n",
    "    query_bundle = QueryBundle(query_str)\n",
    "\n",
    "    retrieved_nodes = retriever.retrieve(query_bundle)\n",
    "\n",
    "    if reranker != \"None\":\n",
    "      retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n",
    "    else:\n",
    "        retrieved_nodes\n",
    "\n",
    "    print(f\"Visualize Retrieved Nodes for Reranker: {rerank_name}\")\n",
    "    visualize_retrieved_nodes(retrieved_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d7bec9-69b6-4a70-9539-b8345566f41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from llama_index.core.response.notebook_utils import display_source_node\n",
    "\n",
    "#for node in retrieved_nodes:\n",
    "    #display_source_node(node, source_length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "98bf59bb-662d-4e08-a71d-9b51a086bce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dac401f-4c2b-4402-b663-487d954ee70d",
   "metadata": {},
   "source": [
    "For the following code visit the path File ~\\AppData\\Roaming\\Python\\Python311\\site-packages\\llama_index\\core\\llama_dataset\\legacy\\embedding.py:87 >> open in notepad and change\n",
    "#####  response = llm.complete(query) to response=llm.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3631c3-d1f1-4fbf-85f9-8e3237d59420",
   "metadata": {},
   "source": [
    "File ~\\AppData\\Roaming\\Python\\Python311\\site-packages\\llama_index\\core\\llama_dataset\\legacy\\embedding.py:87, in generate_qa_embedding_pairs(nodes, llm, qa_generate_prompt_tmpl, num_questions_per_chunk)\n",
    "     83 for node_id, text in tqdm(node_dict.items()):\n",
    "     84     query = qa_generate_prompt_tmpl.format(\n",
    "     85         context_str=text, num_questions_per_chunk=num_questions_per_chunk\n",
    "     86     )\n",
    "\n",
    "---> 87     response = llm.complete(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd307ff8-6bee-4fbf-af7a-8251dfa13973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [00:12<00:00,  3.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# Prompt to generate questions\n",
    "from llama_index.core.evaluation import (\n",
    "    generate_question_context_pairs,\n",
    "    EmbeddingQAFinetuneDataset,\n",
    ")\n",
    "qa_generate_prompt_tmpl = \"\"\"\\\n",
    "Context information is below.\n",
    "\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "\n",
    "Given the context information and not prior knowledge.\n",
    "generate only questions based on the below query.\n",
    "\n",
    "You are a Professor. Your task is to setup \\\n",
    "{num_questions_per_chunk} questions for an upcoming \\\n",
    "quiz/examination. The questions should be diverse in nature \\\n",
    "across the document. The questions should not contain options, not start with Q1/ Q2. \\\n",
    "Restrict the questions to the context information provided.\\\n",
    "\"\"\"\n",
    "\n",
    "# Evaluator\n",
    "\n",
    "qa_dataset = generate_question_context_pairs(nodes, llm=llm, num_questions_per_chunk=2, qa_generate_prompt_tmpl=qa_generate_prompt_tmpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9126ae7a-ef08-4250-b3ac-492debace24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#qa_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ff26685f-ad75-4250-92a4-505f84a05e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for displaying results\n",
    "def display_results(reranker_name, eval_results):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "    hit_rate = full_df[\"hit_rate\"].mean()\n",
    "    mrr = full_df[\"mrr\"].mean()\n",
    "\n",
    "    metric_df = pd.DataFrame({\"Reranker\": [reranker_name], \"hit_rate\": [hit_rate], \"mrr\": [mrr]})\n",
    "\n",
    "    return metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "69c1a527-121a-408f-b594-4f0e58cff929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Evaluation for Reranker: WithoutReranker\n",
      "Running Evaluation for Reranker: CohereRerank\n",
      "Running Evaluation for Reranker: bge-reranker-base\n",
      "Running Evaluation for Reranker: bge-reranker-large\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "query_str = \"Different finetuning methods and their memory requirements?\"\n",
    "\n",
    "results_df = pd.DataFrame()\n",
    "# Loop over rerankers\n",
    "for rerank_name, reranker in RERANKERS.items():\n",
    "    print(f\"Running Evaluation for Reranker: {rerank_name}\")\n",
    "\n",
    "    query_bundle = QueryBundle(query_str)\n",
    "\n",
    "    retrieved_nodes = retriever.retrieve(query_bundle)\n",
    "\n",
    "    if reranker != \"None\":\n",
    "      retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n",
    "    else:\n",
    "        retrieved_nodes\n",
    "\n",
    "    retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "        [\"mrr\", \"hit_rate\"], retriever=retriever\n",
    "    )\n",
    "\n",
    "    eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)\n",
    "\n",
    "    current_df = display_results(rerank_name, eval_results)\n",
    "    results_df = pd.concat([results_df, current_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ee1db3d1-d0f5-4026-a4f1-12cbdfb98bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reranker</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WithoutReranker</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.073224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CohereRerank</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.073224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bge-reranker-base</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.073224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bge-reranker-large</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.073224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Reranker  hit_rate       mrr\n",
       "0     WithoutReranker      0.25  0.073224\n",
       "1        CohereRerank      0.25  0.073224\n",
       "2   bge-reranker-base      0.25  0.073224\n",
       "3  bge-reranker-large      0.25  0.073224"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8b6833b8-e6ad-46d4-9f3f-fea1c468c0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b716f8ac-cdf4-4541-a5e3-c1abc27c4f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d639c966-22c9-414b-b666-fd85674683d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81a1db24-667e-4b3a-86c2-7cbafe5d06be",
   "metadata": {},
   "source": [
    "# Only Querying with retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5b91c1-0d12-4431-aa36-ceb1e6450897",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_nodes = retriever.retrieve(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c457b8-aa37-4aee-83ea-55a36ee7b401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "qa_prompt = PromptTemplate(\n",
    "    \"\"\"\\\n",
    "Context information is below.\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {query_str}\n",
    "Answer: \\\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62c9215-ccb4-4c4b-af65-a9e3a80e328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(retrieved_nodes, query_str, qa_prompt, llm):\n",
    "    context_str = \"\\n\\n\".join([r.get_content() for r in retrieved_nodes])\n",
    "    fmt_qa_prompt = qa_prompt.format(\n",
    "        context_str=context_str, query_str=query_str\n",
    "    )\n",
    "    response = llm.invoke(fmt_qa_prompt)\n",
    "    return str(response), fmt_qa_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5876f52d-2b18-4d3e-a176-3864fb783f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response, fmt_qa_prompt = generate_response(retrieved_nodes, query_str, qa_prompt, llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb82d4f-7efa-4d44-a297-f4b8d37908f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"*****Response******:\\n{response}\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
